{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Hbol",
      "metadata": {
        "id": "Hbol"
      },
      "source": [
        "# Ridership Open Lakehouse Demo\n",
        "\n",
        "This notebook will demonstrate a strategy to implement an open lakehouse on GCP, using Apache Iceberg,\n",
        "as an open source standard for managing data, while still leveraging GCP native capabilities. This demo will use\n",
        "BigQuery Manged Iceberg Tables, Managed Apache Kafka and Apache Kafka Connect to ingest streaming data, Vertex AI for Generative AI queries on top of the data and Dataplex to govern tables.\n",
        "\n",
        "This notebook will load data into BigQuery, backed by Parquet files, in the Apache Iceberg specification.\n",
        "\n",
        "After loading, we will demonstrate processing the data using PySpark.\n",
        "\n",
        "The processing will simulate `bus ridership` data, based on `bus station ridership` data. The `bus station ridership` shows passangers waiting at a given station at a given timestamp. Our PySpark processing pipelines will create a time windows, simulating a bus picking up those passangers while driving it's route. The routes for the buses are taken from the pre-made `bus_lines` table.\n",
        "\n",
        "All data in this notebook was prepared in the previous `part0` notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MJUe",
      "metadata": {
        "id": "MJUe",
        "marimo": {
          "config": {
            "hide_code": true
          }
        }
      },
      "source": [
        "## Setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "vblA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vblA",
        "outputId": "8451fe24-e8c7-45e1-e77b-2f05b82af43d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lakehouse-demo-1000\n",
            "lakehouse-demo-1000-ridership-lakehouse\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "USER_AGENT = \"cloud-solutions/data-to-ai-nb-v3\"\n",
        "\n",
        "PROJECT_ID = !gcloud config get-value project\n",
        "PROJECT_ID = PROJECT_ID[0]\n",
        "BQ_DATASET = \"ridership_lakehouse\"\n",
        "BUCKET_NAME = f\"{PROJECT_ID}-ridership-lakehouse\"\n",
        "LOCATION = \"us-central1\"\n",
        "BQ_CONNECTION_NAME = \"cloud-resources-connection\"\n",
        "\n",
        "print(PROJECT_ID)\n",
        "print(BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "lEQa",
      "metadata": {
        "id": "lEQa"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery, storage\n",
        "from google.api_core.client_info import ClientInfo\n",
        "\n",
        "bigquery_client = bigquery.Client(\n",
        "    project=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        "    client_info=ClientInfo(user_agent=USER_AGENT)\n",
        ")\n",
        "storage_client = storage.Client(\n",
        "    project=PROJECT_ID,\n",
        "    client_info=ClientInfo(user_agent=USER_AGENT)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OWx9xx9aWpGu",
      "metadata": {
        "id": "OWx9xx9aWpGu"
      },
      "source": [
        "## Create the tables and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Xref",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xref",
        "outputId": "5983ac7e-b80e-4291-d998-ffa6ac79549e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<google.cloud.bigquery.table._EmptyRowIterator at 0x7ea77634d590>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bus_stops_uri = f\"gs://{BUCKET_NAME}/iceberg_data/bus_stations/\"\n",
        "bus_lines_uri = f\"gs://{BUCKET_NAME}/iceberg_data/bus_lines/\"\n",
        "ridership_uri = f\"gs://{BUCKET_NAME}/iceberg_data/ridership/\"\n",
        "\n",
        "bigquery_client.query(f\"DROP TABLE IF EXISTS {BQ_DATASET}.bus_stations;\").result()\n",
        "query = f\"\"\"\n",
        "CREATE TABLE {BQ_DATASET}.bus_stations\n",
        "(\n",
        "  bus_stop_id INTEGER,\n",
        "  address STRING,\n",
        "  school_zone BOOLEAN,\n",
        "  seating BOOLEAN,\n",
        "  latitude FLOAT64,\n",
        "  longtitude FLOAT64\n",
        ")\n",
        "WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_CONNECTION_NAME}`\n",
        "OPTIONS (\n",
        "  file_format = 'PARQUET',\n",
        "  table_format = 'ICEBERG',\n",
        "  storage_uri = '{bus_stops_uri}');\n",
        "\"\"\"\n",
        "bigquery_client.query(query).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "SFPL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFPL",
        "outputId": "0c1ab6fb-264e-434c-95de-1cf1128d09f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<google.cloud.bigquery.table._EmptyRowIterator at 0x7ea764d8b990>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigquery_client.query(\n",
        "    f'DROP TABLE IF EXISTS {BQ_DATASET}.bus_lines;'\n",
        ").result()\n",
        "_create_table_stmt = f\"\"\"\n",
        "    CREATE TABLE {BQ_DATASET}.bus_lines (\n",
        "        bus_line_id INTEGER,\n",
        "        bus_line STRING,\n",
        "        number_of_stops INTEGER,\n",
        "        stops ARRAY<INTEGER>,\n",
        "        frequency_minutes INTEGER\n",
        "    )\n",
        "    WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_CONNECTION_NAME}`\n",
        "    OPTIONS (\n",
        "        file_format = 'PARQUET',\n",
        "        table_format = 'ICEBERG',\n",
        "        storage_uri = '{bus_lines_uri}'\n",
        "    );\n",
        "\"\"\"\n",
        "bigquery_client.query(_create_table_stmt).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "BYtC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYtC",
        "outputId": "5a40049f-0a11-40e5-d6e7-5a8afccf7e87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<google.cloud.bigquery.table._EmptyRowIterator at 0x7ea72c854350>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigquery_client.query(\n",
        "    f'DROP TABLE IF EXISTS {BQ_DATASET}.ridership;'\n",
        ").result()\n",
        "_create_table_stmt = f\"\"\"\n",
        "    CREATE TABLE {BQ_DATASET}.ridership (\n",
        "        transit_timestamp TIMESTAMP,\n",
        "        station_id INTEGER,\n",
        "        ridership INTEGER\n",
        "    )\n",
        "    WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_CONNECTION_NAME}`\n",
        "    OPTIONS (\n",
        "        file_format = 'PARQUET',\n",
        "        table_format = 'ICEBERG',\n",
        "        storage_uri = '{ridership_uri}'\n",
        "    );\n",
        "\"\"\"\n",
        "bigquery_client.query(_create_table_stmt).result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "Kclp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kclp",
        "outputId": "bc093b4b-8a74-4e41-e5e3-f74cf50d8f5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LoadJob<project=lakehouse-demo-1000, location=us-central1, id=6a96e8e1-7c43-4f3c-9005-a0528afc0ca6>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_ref = bigquery_client.dataset(BQ_DATASET)\n",
        "table_ref = dataset_ref.table(\"bus_lines\")\n",
        "\n",
        "# BQ tables for Apache Iceberg do not support load with truncating, so we will truncate manually, and then load\n",
        "truncate = bigquery_client.query(f\"DELETE FROM {BQ_DATASET}.bus_lines WHERE TRUE\")\n",
        "truncate.result()\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
        "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
        ")\n",
        "\n",
        "job = bigquery_client.load_table_from_uri(\n",
        "    f\"gs://{BUCKET_NAME}/mta_staging_data/bus_lines.json\",\n",
        "    table_ref,\n",
        "    job_config=job_config,\n",
        ")\n",
        "\n",
        "job.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "emfo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emfo",
        "outputId": "c5c3ae35-3134-43cf-eabd-c91146aa06a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LoadJob<project=lakehouse-demo-1000, location=us-central1, id=1f529f45-11d1-4644-9ce8-abd1990156b0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table_ref = dataset_ref.table(\"bus_stations\")\n",
        "\n",
        "# BQ tables for Apache Iceberg do not support load with truncating, so we will truncate manually, and then load\n",
        "truncate = bigquery_client.query(f\"DELETE FROM {BQ_DATASET}.bus_stations WHERE TRUE\")\n",
        "truncate.result()\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        ")\n",
        "\n",
        "job = bigquery_client.load_table_from_uri(\n",
        "    f\"gs://{BUCKET_NAME}/mta_staging_data/bus_stations.csv\",\n",
        "    table_ref,\n",
        "    job_config=job_config,\n",
        ")\n",
        "\n",
        "job.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "Hstk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hstk",
        "outputId": "18951878-eed2-4e9e-cf56-9350b66b6a92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LoadJob<project=lakehouse-demo-1000, location=us-central1, id=7f3dd344-6687-4fcd-819c-289b51ed9594>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table_ref = dataset_ref.table(\"ridership\")\n",
        "\n",
        "# BQ tables for Apache Iceberg do not support load with truncating, so we will truncate manually, and then load\n",
        "truncate = bigquery_client.query(f\"DELETE FROM {BQ_DATASET}.ridership WHERE TRUE\")\n",
        "truncate.result()\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        ")\n",
        "\n",
        "job = bigquery_client.load_table_from_uri(\n",
        "    f\"gs://{BUCKET_NAME}/mta_staging_data/ridership/*.csv\",\n",
        "    table_ref,\n",
        "    job_config=job_config,\n",
        ")\n",
        "\n",
        "job.result()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e26e3bd",
      "metadata": {},
      "source": [
        "## Data Processing\n",
        "After loading the data to our open data lakehouse, we process the raw data with Dataproc. We simulate bus trips with arrivals and stops along with passengers going in and out of the bus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d5cd356",
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/usr/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = !gcloud config get-value project\n",
        "PROJECT_ID = PROJECT_ID[0]\n",
        "\n",
        "BQ_DATASET = \"ridership_lakehouse\"\n",
        "LOCATION = \"us-central1\"\n",
        "APP_NAME = \"open-data-lakehouse-demo\"\n",
        "\n",
        "print(PROJECT_ID)\n",
        "print(LOCATION)\n",
        "print(BQ_DATASET)\n",
        "print(APP_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31984333",
      "metadata": {},
      "source": [
        "### Data Processing Function Definintions\n",
        "Define the functions that execute the data processing:\n",
        "- Simulate bus stops and arrivals\n",
        "- Join the corresponding ridership data and simulate the passenger flow\n",
        "- Save the completed trips to BQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "634c1141",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install dataproc-spark-connect\n",
        "\n",
        "from google.cloud.dataproc_v1 import Session\n",
        "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
        "from datetime import datetime, timedelta\n",
        "from pyspark.sql.connect.functions import when, col, lag, unix_timestamp, window, floor, rand, sum as Fsum, expr\n",
        "from pyspark.sql.connect.window import Window\n",
        "from pyspark.sql.connect.types import StructType, StructField, LongType\n",
        "\n",
        "\n",
        "session_config = Session()\n",
        "\n",
        "# Create the Spark session.\n",
        "spark = DataprocSparkSession.builder.projectId(f\"{PROJECT_ID}\").appName(f\"{APP_NAME}\").location(f\"{LOCATION}\").dataprocSessionConfig(session_config).getOrCreate()\n",
        "\n",
        "ridership = spark.read.format('bigquery') \\\n",
        "    .option(\"project\", PROJECT_ID) \\\n",
        "    .option(\"table\", f\"{PROJECT_ID}:{BQ_DATASET}.ridership\") \\\n",
        "    .load()\n",
        "\n",
        "ridership = ridership.withColumn(\"transit_timestamp_unix_ts\", unix_timestamp(\"transit_timestamp\"))\n",
        "ridership.createOrReplaceTempView(\"ridership_view\")\n",
        "\n",
        "\n",
        "def get_bus_lines_stations():\n",
        "    \"\"\"\n",
        "    Returns a DataFrame of the ordererd stations for all bus lines.\n",
        "    \"\"\"\n",
        "\n",
        "    df_table = spark.read.format(\"bigquery\") \\\n",
        "        .option(\"project\", PROJECT_ID) \\\n",
        "        .option(\"table\", f\"{PROJECT_ID}:{BQ_DATASET}.bus_lines\") \\\n",
        "        .load()\n",
        "\n",
        "\n",
        "    unnested_stations = df_table.selectExpr(\"bus_line_id\", \"posexplode(stops) as (stop_index, stop_value)\")\n",
        "    bus_lines_stations = unnested_stations.selectExpr(\"bus_line_id\", \"stop_value AS stop_id\", \"stop_index AS stop_index\")\n",
        "    bus_lines_stations.createOrReplaceTempView(\"bus_stations\")\n",
        "\n",
        "    return bus_lines_stations\n",
        "\n",
        "\n",
        "def get_bus_trips_simulation(start_time_str, min_trip_duration_minutes=1, max_trip_duration_minutes=3, seating_capacity_list=[10, 20, 25], standing_capacity_list=[20,30,35]):\n",
        "    \"\"\"\n",
        "    Simulates trips with arrival times for each station based on the bus line schedules and a starting time.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = datetime.strptime(start_time_str, \"%Y-%m-%d %H:%M:%S\")\n",
        "    unix_ts = int(start_time.timestamp())\n",
        "    duration_range = max_trip_duration_minutes - min_trip_duration_minutes + 1\n",
        "    start_ts_str = start_time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    capacity_mapping = \",\\n    \".join([\n",
        "        f\"({i}, {seating_capacity_list[i]}, {standing_capacity_list[i]})\"\n",
        "        for i in range(min(len(seating_capacity_list), len(standing_capacity_list)))\n",
        "    ])\n",
        "    num_options = len(seating_capacity_list)\n",
        "\n",
        "    # Query\n",
        "    query = f\"\"\"\n",
        "    WITH capacities_list AS (\n",
        "      SELECT * FROM VALUES\n",
        "        {capacity_mapping}\n",
        "      AS t(idx, seating_capacity, standing_capacity)\n",
        "    ),\n",
        "\n",
        "    bus_line_assignments AS (\n",
        "      SELECT\n",
        "        bus_line_id,\n",
        "        ROW_NUMBER() OVER (ORDER BY bus_line_id) % {num_options} AS idx\n",
        "      FROM (\n",
        "        SELECT DISTINCT bus_line_id FROM bus_stations\n",
        "      )\n",
        "    ),\n",
        "\n",
        "    bus_capacities AS (\n",
        "      SELECT\n",
        "        a.bus_line_id,\n",
        "        c.seating_capacity,\n",
        "        c.standing_capacity\n",
        "      FROM bus_line_assignments a\n",
        "      JOIN capacities_list c ON a.idx = c.idx\n",
        "    ),\n",
        "\n",
        "    delays AS (\n",
        "      SELECT\n",
        "        bus_line_id,\n",
        "        stop_id,\n",
        "        stop_index,\n",
        "        FLOOR((ABS(HASH(bus_line_id, stop_index)) % {duration_range})) + {min_trip_duration_minutes} AS random_delay_min,\n",
        "        SUM(FLOOR((ABS(HASH(bus_line_id, stop_index)) % {duration_range})) + {min_trip_duration_minutes}) OVER (\n",
        "          PARTITION BY bus_line_id\n",
        "          ORDER BY stop_index\n",
        "          ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
        "        ) AS cumulative_delay_min,\n",
        "        MAX(stop_index) OVER (PARTITION BY bus_line_id) AS max_stop_index\n",
        "      FROM bus_stations\n",
        "    ),\n",
        "\n",
        "    base AS (\n",
        "      SELECT UNIX_SECONDS(TIMESTAMP('{start_ts_str}')) AS start_ts_unix\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "      trip_id,\n",
        "      d.bus_line_id,\n",
        "      d.stop_id,\n",
        "      d.stop_index,\n",
        "      d.random_delay_min,\n",
        "      d.cumulative_delay_min,\n",
        "      arrival_time,\n",
        "      CAST(arrival_time AS LONG) AS arrival_time_unix_ts,\n",
        "      c.seating_capacity,\n",
        "      c.standing_capacity,\n",
        "      (d.stop_index = d.max_stop_index) AS is_last_stop\n",
        "    FROM (\n",
        "      SELECT\n",
        "        {unix_ts} as trip_id,\n",
        "        d.bus_line_id,\n",
        "        d.stop_id,\n",
        "        d.stop_index,\n",
        "        d.random_delay_min,\n",
        "        d.cumulative_delay_min,\n",
        "        d.max_stop_index,\n",
        "        TIMESTAMP_SECONDS(b.start_ts_unix + d.cumulative_delay_min * 60) AS arrival_time\n",
        "      FROM delays d\n",
        "      CROSS JOIN base b\n",
        "    ) d\n",
        "    JOIN bus_capacities c ON d.bus_line_id = c.bus_line_id\n",
        "    ORDER BY d.bus_line_id, d.stop_index\n",
        "    \"\"\"\n",
        "\n",
        "    return spark.sql(query)\n",
        "\n",
        "\n",
        "def get_ridership_window_intervals(step_in_minutes=10, window_size_in_minutes=180):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame that constructs the timestamp window intervals based on the ridership_view\n",
        "    \"\"\"\n",
        "\n",
        "    step = step_in_minutes * 60\n",
        "    window_size = window_size_in_minutes * 60\n",
        "\n",
        "    #CAN THIS BE IMPROVED?\n",
        "    timestamps_row = spark.sql(\"\"\"\n",
        "        SELECT\n",
        "            unix_timestamp(MIN(transit_timestamp)) AS start_ts,\n",
        "            unix_timestamp(MAX(transit_timestamp)) AS end_ts\n",
        "        FROM ridership_view\n",
        "    \"\"\").first()\n",
        "\n",
        "    start_ts = timestamps_row[\"start_ts\"]\n",
        "    end_ts = timestamps_row[\"end_ts\"]\n",
        "\n",
        "    window_bounds = []\n",
        "    ts = start_ts\n",
        "    while ts + window_size <= end_ts:\n",
        "        window_bounds.append((ts, ts + window_size))\n",
        "        ts += step\n",
        "\n",
        "    # Create a DataFrame from the list of windows\n",
        "    window_schema = StructType([\n",
        "        StructField(\"window_start\", LongType(), False),\n",
        "        StructField(\"window_end\", LongType(), False),\n",
        "    ])\n",
        "\n",
        "    window_df = spark.createDataFrame(window_bounds, schema=window_schema)\n",
        "\n",
        "    return window_df\n",
        "\n",
        "\n",
        "def get_ridership_window(ridership_df, window_start, window_end):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame that represents the ridership_df window given by the window start and end\n",
        "    \"\"\"\n",
        "\n",
        "    ridership_window_data = ridership_df.filter(\n",
        "      (col(\"transit_timestamp_unix_ts\") >= window_start) &\n",
        "      (col(\"transit_timestamp_unix_ts\") < window_end)\n",
        "    )\n",
        "\n",
        "    return ridership_window_data\n",
        "\n",
        "\n",
        "def get_ridership_simulation_data_filtered(simulations_df, ridership_df, window_start, window_end):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame that represents the station arrivals simulation populated with the ridership data, filtered\n",
        "    to include only the time span between the window start and window end of the data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Alias both DataFrames to disambiguate columns\n",
        "    sim_df = simulations_df.alias(\"sim\")\n",
        "    rid_df = ridership_df.alias(\"rid\")\n",
        "\n",
        "    # Perform join with aliased references\n",
        "    simulations_with_ridership = sim_df.join(\n",
        "        rid_df,\n",
        "        on=(\n",
        "            (col(\"sim.arrival_time_unix_ts\") == col(\"rid.transit_timestamp_unix_ts\")) &\n",
        "            (col(\"sim.stop_id\") == col(\"rid.station_id\"))\n",
        "        ),\n",
        "        how=\"left\"\n",
        "    ).select(\n",
        "        col(\"sim.trip_id\"),\n",
        "        col(\"sim.bus_line_id\"),\n",
        "        col(\"sim.stop_id\"),\n",
        "        col(\"sim.stop_index\"),\n",
        "        col(\"sim.is_last_stop\"),\n",
        "        col(\"sim.random_delay_min\"),\n",
        "        col(\"sim.cumulative_delay_min\"),\n",
        "        col(\"rid.ridership\"),\n",
        "        col(\"sim.arrival_time\"),\n",
        "        col(\"sim.arrival_time_unix_ts\"),\n",
        "        col(\"sim.seating_capacity\"),\n",
        "        col(\"sim.standing_capacity\")\n",
        "    )\n",
        "\n",
        "    # Filter based on time window\n",
        "    filtered_simulations_with_ridership = simulations_with_ridership.filter(\n",
        "        (col(\"arrival_time_unix_ts\") >= window_start) &\n",
        "        (col(\"arrival_time_unix_ts\") < window_end)\n",
        "    )\n",
        "\n",
        "    return filtered_simulations_with_ridership\n",
        "\n",
        "\n",
        "def aggregate_ridership_and_simulate_capacity_data (simulations_with_ridership_df):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame that performs:\n",
        "      - random passengers out simulations based on previous station passengers in\n",
        "      - calculates bus onboard passangers\n",
        "      - decides if the buses are overloaded or not\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    w = Window.partitionBy(\"trip_id\", \"bus_line_id\").orderBy(\"stop_index\")\n",
        "\n",
        "    cumulative_sum = Fsum(when(col(\"ridership\").isNotNull(), col(\"ridership\"))).over(w)\n",
        "\n",
        "    simulations_with_ridership_df = simulations_with_ridership_df.withColumn(\n",
        "        \"cumulative_ridership\",\n",
        "        when(col(\"ridership\").isNotNull(), cumulative_sum)\n",
        "    )\n",
        "\n",
        "    simulations_with_ridership_df = simulations_with_ridership_df.withColumn(\"prev_ridership\", lag(col(\"ridership\")).over(w))\n",
        "\n",
        "    #We might have to implement a more random passengers_out column. Right now it's based on a random value of the previous ridership value.\n",
        "    simulations_with_ridership_df = simulations_with_ridership_df.withColumn(\n",
        "      \"passengers_out\",\n",
        "      when(\n",
        "          col(\"stop_index\") == 0,  # if first stop\n",
        "          0\n",
        "      ).otherwise(\n",
        "          when(\n",
        "              col(\"ridership\").isNotNull(),\n",
        "              floor(\n",
        "                  rand() * (when(col(\"prev_ridership\").isNotNull(), col(\"prev_ridership\")).otherwise(col(\"ridership\")) + 1)\n",
        "              )\n",
        "          ).otherwise(None)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    w_currentRow = Window.partitionBy(\"trip_id\", \"bus_line_id\").orderBy(\"stop_index\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "    simulations_with_ridership_df = simulations_with_ridership_df.withColumn(\n",
        "        \"passengers_onboard_at_departure\",\n",
        "        when(\n",
        "            col(\"ridership\").isNull(),\n",
        "            None\n",
        "        ).otherwise(\n",
        "            Fsum(col(\"ridership\") - col(\"passengers_out\")).over(w_currentRow)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    simulations_with_ridership_df = simulations_with_ridership_df.withColumn(\n",
        "        \"seating_overcapacity\",\n",
        "        when(\n",
        "            col(\"passengers_onboard_at_departure\").isNotNull() &\n",
        "            (col(\"passengers_onboard_at_departure\") > col(\"seating_capacity\")),\n",
        "            True\n",
        "        ).otherwise(False)\n",
        "    ).withColumn(\n",
        "        \"standing_overcapacity\",\n",
        "        when(\n",
        "            col(\"passengers_onboard_at_departure\").isNotNull() &\n",
        "            (col(\"passengers_onboard_at_departure\") > (col(\"seating_capacity\") + col(\"standing_capacity\"))),\n",
        "            True\n",
        "        ).otherwise(False)\n",
        "    )\n",
        "\n",
        "    return simulations_with_ridership_df\n",
        "\n",
        "\n",
        "def handle_complete_arrival_simulation_data(simulations_with_ridership_df, window_end):\n",
        "    \"\"\"\n",
        "    Decides which trips from the simulations_with_ridership_df had the chance to be populated with ridership data and saves them to BQ.\n",
        "    \"\"\"\n",
        "\n",
        "    # Filter out qualifying trips that reached the last stop before the window_end\n",
        "    qualifying_trip_ids_df = simulations_with_ridership_df.filter(\n",
        "        (col(\"is_last_stop\") == True) &\n",
        "        (col(\"arrival_time_unix_ts\") < window_end)\n",
        "    ).select(\"trip_id\", \"bus_line_id\").distinct()\n",
        "\n",
        "    # Reconstruct full rows for those qualifying trips\n",
        "    qualifying_trips_df = simulations_with_ridership_df.alias(\"sim\").join(\n",
        "        qualifying_trip_ids_df.alias(\"qual_ids\"),\n",
        "        on=[\"trip_id\", \"bus_line_id\"],\n",
        "        how=\"inner\"\n",
        "    ).select(\n",
        "        col(\"sim.trip_id\"),\n",
        "        col(\"sim.bus_line_id\"),\n",
        "        col(\"sim.stop_id\"),\n",
        "        col(\"sim.stop_index\"),\n",
        "        col(\"sim.is_last_stop\"),\n",
        "        col(\"sim.random_delay_min\"),\n",
        "        col(\"sim.cumulative_delay_min\"),\n",
        "        col(\"sim.ridership\"),\n",
        "        col(\"sim.arrival_time\"),\n",
        "        col(\"sim.arrival_time_unix_ts\"),\n",
        "        col(\"sim.seating_capacity\"),\n",
        "        col(\"sim.standing_capacity\")\n",
        "    )\n",
        "\n",
        "    # Enrich with ridership aggregation\n",
        "    qualifying_trips_df = aggregate_ridership_and_simulate_capacity_data(qualifying_trips_df)\n",
        "    \n",
        "    # Write the qualifying trips BigQuery\n",
        "    qualifying_trips_df.write \\\n",
        "        .format(\"bigquery\") \\\n",
        "        .option(\"table\", f\"{PROJECT_ID}:{BQ_DATASET}.simulation\") \\\n",
        "        .option(\"writeMethod\", \"direct\") \\\n",
        "        .mode(\"append\") \\\n",
        "        .save()    \n",
        "\n",
        "    return qualifying_trips_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d5934b",
      "metadata": {},
      "source": [
        "### Data Processing Run\n",
        "Define the functions that execute the data processing:\n",
        "- Simulate bus stops and arrivals\n",
        "- Join the corresponding ridership data and simulate the passenger flow\n",
        "- Save the completed trips to BQ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0d0958",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "\n",
        "bus_lines_stations = get_bus_lines_stations()\n",
        "bus_lines_stations.cache()\n",
        "\n",
        "intervals_df = get_ridership_window_intervals()\n",
        "intervals_df_f=intervals_df.filter(intervals_df.window_end<=1593583800)\n",
        "\n",
        "for row in intervals_df_f.sort(\"window_start\").toLocalIterator():\n",
        "    window_start = int(row['window_start'])\n",
        "    window_end = int(row['window_end'])\n",
        "\n",
        "    window_start_str = datetime.fromtimestamp(window_start, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    #1. get filter_ridership\n",
        "    ridership_window_data=get_ridership_window(ridership, window_start, window_end)\n",
        "\n",
        "    #2. arrivals_simulation for the lower end of the interval\n",
        "    simulations=get_bus_trips_simulation(window_start_str)\n",
        "\n",
        "    #3. join and create a filtered dataframe\n",
        "    simulations_with_ridership=get_ridership_simulation_data_filtered(simulations, ridership_window_data, window_start, window_end)\n",
        "\n",
        "    #4. join and create a filtered dataframe\n",
        "    complete_simulations_with_ridership = handle_complete_arrival_simulation_data(simulations_with_ridership, window_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nWHF",
      "metadata": {
        "id": "nWHF",
        "marimo": {
          "config": {
            "hide_code": true
          }
        }
      },
      "source": [
        "## Basic Analytics\n",
        "After loading the data to our open data lakehouse, we will demonstrate some basic analytics, but we will repeat the process with several different engines\n",
        "- BigQuery\n",
        "- Spark (serverless?)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "lakehouse_part1_load_data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
