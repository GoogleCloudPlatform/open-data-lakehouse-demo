{"cells": [{"cell_type": "code", "source": ["# Copyright 2025 Google LLC\n", "#\n", "# Licensed under the Apache License, Version 2.0 (the \"License\");\n", "# you may not use this file except in compliance with the License.\n", "# You may obtain a copy of the License at\n", "#\n", "#     https://www.apache.org/licenses/LICENSE-2.0\n", "#\n", "# Unless required by applicable law or agreed to in writing, software\n", "# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n", "# See the License for the specific language governing permissions and\n", "# limitations under the License."], "metadata": {"id": "3rDg9KfcBdJP"}, "id": "3rDg9KfcBdJP", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "Hbol", "metadata": {"id": "Hbol"}, "source": ["# Ridership Open Lakehouse Demo (Part 1): Load data to BigQuery Iceberg tables\n", "\n", "This notebook will demonstrate a strategy to implement an open lakehouse on GCP, using Apache Iceberg,\n", "as an open source standard for managing data, while still leveraging GCP native capabilities. This demo will use\n", "BigQuery Manged Iceberg Tables, Managed Apache Kafka and Apache Kafka Connect to ingest streaming data, Vertex AI for Generative AI queries on top of the data and Dataplex to govern tables.\n", "\n", "This notebook will load data into BigQuery, backed by Parquet files, in the Apache Iceberg specification.\n", "\n", "All data in this notebook was prepared in the previous `part0` notebook."], "execution_count": null}, {"cell_type": "markdown", "id": "MJUe", "metadata": {"marimo": {"config": {"hide_code": true}}, "id": "MJUe"}, "source": ["## Setup the environment"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "vblA", "metadata": {"id": "vblA", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "41bf4858-d11c-460d-b368-68d8688823f8"}, "outputs": [], "source": ["import os\n", "USER_AGENT = \"cloud-solutions/data-to-ai-nb-v3\"\n", "\n", "PROJECT_ID = !gcloud config get-value project\n", "PROJECT_ID = PROJECT_ID[0]\n", "BQ_DATASET = \"ridership_lakehouse\"\n", "BUCKET_NAME = f\"{PROJECT_ID}-ridership-lakehouse\"\n", "LOCATION = \"us-central1\"\n", "BQ_CONNECTION_NAME = \"cloud-resources-connection\"\n", "\n", "print(PROJECT_ID)\n", "print(BUCKET_NAME)"]}, {"cell_type": "code", "execution_count": null, "id": "lEQa", "metadata": {"id": "lEQa"}, "outputs": [], "source": ["from google.cloud import bigquery, storage\n", "from google.api_core.client_info import ClientInfo\n", "\n", "bigquery_client = bigquery.Client(\n", "    project=PROJECT_ID,\n", "    location=LOCATION,\n", "    client_info=ClientInfo(user_agent=USER_AGENT)\n", ")\n", "storage_client = storage.Client(\n", "    project=PROJECT_ID,\n", "    client_info=ClientInfo(user_agent=USER_AGENT)\n", ")"]}, {"cell_type": "markdown", "source": ["## Create the tables and load data"], "metadata": {"id": "OWx9xx9aWpGu"}, "id": "OWx9xx9aWpGu", "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "Xref", "metadata": {"id": "Xref", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "b3a11b45-1a3a-4b3c-8c8f-8d89ea5de219"}, "outputs": [], "source": ["bus_stops_uri = f\"gs://{BUCKET_NAME}/iceberg_data/bus_stations/\"\n", "\n", "bigquery_client.query(f\"DROP TABLE IF EXISTS {BQ_DATASET}.bus_stations;\").result()\n", "query = f\"\"\"\n", "CREATE TABLE {BQ_DATASET}.bus_stations\n", "(\n", "  bus_stop_id INTEGER,\n", "  address STRING,\n", "  school_zone BOOLEAN,\n", "  seating BOOLEAN,\n", "  borough STRING,\n", "  latitude FLOAT64,\n", "  longtitude FLOAT64\n", ")\n", "WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_CONNECTION_NAME}`\n", "OPTIONS (\n", "  file_format = 'PARQUET',\n", "  table_format = 'ICEBERG',\n", "  storage_uri = '{bus_stops_uri}');\n", "\"\"\"\n", "bigquery_client.query(query).result()"]}, {"cell_type": "code", "execution_count": null, "id": "SFPL", "metadata": {"id": "SFPL", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "0875fce3-6a8a-4cf8-f270-7ae441a7091d"}, "outputs": [], "source": ["bus_lines_uri = f\"gs://{BUCKET_NAME}/iceberg_data/bus_lines/\"\n", "\n", "bigquery_client.query(\n", "    f'DROP TABLE IF EXISTS {BQ_DATASET}.bus_lines;'\n", ").result()\n", "_create_table_stmt = f\"\"\"\n", "    CREATE TABLE {BQ_DATASET}.bus_lines (\n", "        bus_line_id INTEGER,\n", "        bus_line STRING,\n", "        number_of_stops INTEGER,\n", "        stops ARRAY<INTEGER>,\n", "        frequency_minutes INTEGER\n", "    )\n", "    WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_CONNECTION_NAME}`\n", "    OPTIONS (\n", "        file_format = 'PARQUET',\n", "        table_format = 'ICEBERG',\n", "        storage_uri = '{bus_lines_uri}'\n", "    );\n", "\"\"\"\n", "bigquery_client.query(_create_table_stmt).result()"]}, {"cell_type": "code", "execution_count": null, "id": "BYtC", "metadata": {"id": "BYtC", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "f7ae9c38-ab65-4787-9c7b-7b40c937394d"}, "outputs": [], "source": ["ridership_uri = f\"gs://{BUCKET_NAME}/iceberg_data/ridership/\"\n", "\n", "bigquery_client.query(\n", "    f'DROP TABLE IF EXISTS {BQ_DATASET}.ridership;'\n", ").result()\n", "_create_table_stmt = f\"\"\"\n", "    CREATE TABLE {BQ_DATASET}.ridership (\n", "        transit_timestamp TIMESTAMP,\n", "        station_id INTEGER,\n", "        ridership INTEGER\n", "    )\n", "    WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_CONNECTION_NAME}`\n", "    OPTIONS (\n", "        file_format = 'PARQUET',\n", "        table_format = 'ICEBERG',\n", "        storage_uri = '{ridership_uri}'\n", "    );\n", "\"\"\"\n", "bigquery_client.query(_create_table_stmt).result()"]}, {"cell_type": "code", "execution_count": null, "id": "Kclp", "metadata": {"id": "Kclp", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "6b7fd07a-e4ea-4012-b4d9-628ec93dcc42"}, "outputs": [], "source": ["dataset_ref = bigquery_client.dataset(BQ_DATASET)\n", "table_ref = dataset_ref.table(\"bus_lines\")\n", "\n", "# BQ tables for Apache Iceberg do not support load with truncating, so we will truncate manually, and then load\n", "truncate = bigquery_client.query(f\"DELETE FROM {BQ_DATASET}.bus_lines WHERE TRUE\")\n", "truncate.result()\n", "\n", "job_config = bigquery.LoadJobConfig(\n", "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n", "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n", ")\n", "\n", "job = bigquery_client.load_table_from_uri(\n", "    f\"gs://{BUCKET_NAME}/mta_staging_data/bus_lines.json\",\n", "    table_ref,\n", "    job_config=job_config,\n", ")\n", "\n", "job.result()"]}, {"cell_type": "code", "execution_count": null, "id": "emfo", "metadata": {"id": "emfo", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "c5d39aa8-68bd-492f-97d2-f9411af78595"}, "outputs": [], "source": ["table_ref = dataset_ref.table(\"bus_stations\")\n", "\n", "# BQ tables for Apache Iceberg do not support load with truncating, so we will truncate manually, and then load\n", "truncate = bigquery_client.query(f\"DELETE FROM {BQ_DATASET}.bus_stations WHERE TRUE\")\n", "truncate.result()\n", "\n", "job_config = bigquery.LoadJobConfig(\n", "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n", "    source_format=bigquery.SourceFormat.CSV,\n", "    skip_leading_rows=1,\n", ")\n", "\n", "job = bigquery_client.load_table_from_uri(\n", "    f\"gs://{BUCKET_NAME}/mta_staging_data/bus_stations.csv\",\n", "    table_ref,\n", "    job_config=job_config,\n", ")\n", "\n", "job.result()"]}, {"cell_type": "code", "execution_count": null, "id": "Hstk", "metadata": {"id": "Hstk", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "772c9357-0dbf-474a-8645-07e635a548ca"}, "outputs": [], "source": ["table_ref = dataset_ref.table(\"ridership\")\n", "\n", "# BQ tables for Apache Iceberg do not support load with truncating, so we will truncate manually, and then load\n", "truncate = bigquery_client.query(f\"DELETE FROM {BQ_DATASET}.ridership WHERE TRUE\")\n", "truncate.result()\n", "\n", "job_config = bigquery.LoadJobConfig(\n", "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n", "    source_format=bigquery.SourceFormat.CSV,\n", "    skip_leading_rows=1,\n", ")\n", "\n", "job = bigquery_client.load_table_from_uri(\n", "    f\"gs://{BUCKET_NAME}/mta_staging_data/ridership/*.csv\",\n", "    table_ref,\n", "    job_config=job_config,\n", ")\n", "\n", "job.result()"]}, {"cell_type": "markdown", "id": "nWHF", "metadata": {"marimo": {"config": {"hide_code": true}}, "id": "nWHF"}, "source": ["## Basic Analytics\n", "After loading the data to our open data lakehouse, we will demonstrate some basic analytics, but we will repeat the process with several different engines\n", "- BigQuery\n", "- Spark (serverless?)\n", "- Dataflow"], "execution_count": null}], "metadata": {"colab": {"provenance": [], "name": "lakehouse_part1_load_data.ipynb"}, "language_info": {"name": "python"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 5}