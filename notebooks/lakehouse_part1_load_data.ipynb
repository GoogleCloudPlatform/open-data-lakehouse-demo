{
  "cells": [
    {
      "cell_type": "code",
      "id": "SDCwk98rjVvHm6PPvN5Wg74K",
      "metadata": {
        "tags": [],
        "id": "SDCwk98rjVvHm6PPvN5Wg74K"
      },
      "source": [
        "# Ridership Open Lakehouse Demo\n",
        "\n",
        "This notebook will demonstrate a strategy to implement an open lakehouse on GCP, using Apache Iceberg, as an open source standard for managing data, while still levergin GCP native capabilities. This demo will use BigQuery Manged Iceberg Tables, Managed Apache Kafka and Apache Kafka Connect to ingest streaming data, Vertex AI for Generative AI queries on top of the data and Dataplex to govern tables.\n",
        "\n",
        "This notebook will load the data generated in the previous notebook to BQ, and setup streaming resources"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the environment"
      ],
      "metadata": {
        "id": "UT_RABXLJ4ux"
      },
      "id": "UT_RABXLJ4ux"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-bigquery google-cloud-aiplatform google-cloud-storage --upgrade --quiet"
      ],
      "metadata": {
        "id": "dYnyTTqfJ7ec"
      },
      "id": "dYnyTTqfJ7ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"your project ID here\" # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "# in-case someone didn't update the project manually, assume current project is the right one\n",
        "if PROJECT_ID == \"your project ID here\":\n",
        "    PROJECT_ID = !gcloud config get-value project\n",
        "    PROJECT_ID = PROJECT_ID[0]\n",
        "\n",
        "BUCKET = f\"{PROJECT_ID}-ridership-lakehouse\" # bucket will be created in a subsequant step\n",
        "SOURCE_DATA_BUCKET = f\"{PROJECT_ID}-ridership-lakehouse\"\n",
        "USER_AGENT = \"cloud-solutions/data-to-ai-nb-v3\"\n",
        "BQ_DATASET = \"ridership_lakehouse\"\n",
        "PROJECT_ID"
      ],
      "metadata": {
        "id": "8wuqFXFKb1gc"
      },
      "id": "8wuqFXFKb1gc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Clients"
      ],
      "metadata": {
        "id": "qfXwURjab3OF"
      },
      "id": "qfXwURjab3OF"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage, bigquery\n",
        "from google.api_core import exceptions\n",
        "from google.api_core.client_info import ClientInfo\n",
        "from google.cloud.exceptions import NotFound\n",
        "\n",
        "bigquery_client = bigquery.Client(\n",
        "    project=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        "    client_info=ClientInfo(user_agent=USER_AGENT)\n",
        ")\n",
        "storage_client = storage.Client(\n",
        "    project=PROJECT_ID,\n",
        "    client_info=ClientInfo(user_agent=USER_AGENT)\n",
        ")\n"
      ],
      "metadata": {
        "id": "-R-uWeDsb6Y9"
      },
      "id": "-R-uWeDsb6Y9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Move the next 5 cells to Terraform"
      ],
      "metadata": {
        "id": "M0jUPXmk47J7"
      },
      "id": "M0jUPXmk47J7"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    bucket = storage_client.create_bucket(BUCKET, location=LOCATION)\n",
        "    print(f\"Bucket {BUCKET} created\")\n",
        "except exceptions.Conflict:\n",
        "    # Bucket already exists - return the existing bucket\n",
        "    bucket = storage_client.bucket(BUCKET)\n",
        "    print(f\"Bucket {BUCKET} already exists\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating bucket {BUCKET}: {e}\")"
      ],
      "metadata": {
        "id": "bjgW2b0ecZj6"
      },
      "id": "bjgW2b0ecZj6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  dataset = bigquery.Dataset(f'{PROJECT_ID}.{BQ_DATASET}')\n",
        "  dataset.location = LOCATION\n",
        "  bigquery_client.get_dataset(BQ_DATASET)\n",
        "  print(\"dataset exists\")\n",
        "except NotFound:\n",
        "  bigquery_client.create_dataset(dataset, timeout=30)\n",
        "  print('dataset created {}'.format(e))\n",
        "\n",
        "dataset_ref = bigquery_client.dataset(BQ_DATASET)"
      ],
      "metadata": {
        "id": "P9-s1pa2ceIn"
      },
      "id": "P9-s1pa2ceIn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bq mk \\\n",
        "--connection \\\n",
        "--location={LOCATION} \\\n",
        "--project_id={PROJECT_ID} \\\n",
        "--connection_type=CLOUD_RESOURCE \\\n",
        " {BQ_DATASET}"
      ],
      "metadata": {
        "id": "KOPc0EbVchCV"
      },
      "id": "KOPc0EbVchCV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "connection_details_json_str = !bq show --format json --connection {PROJECT_ID}.{LOCATION}.{BQ_DATASET}\n",
        "connection_details_dict = json.loads(connection_details_json_str[0])\n",
        "CONNECTION_SA_ID = connection_details_dict[\"cloudResource\"][\"serviceAccountId\"]\n",
        "if not CONNECTION_SA_ID:\n",
        "    # it's possible that this command failed, when ran immediately after the previous command\n",
        "    # this is due to the time it takes the API to be consistent due to async actions on GCP\n",
        "    # we will wait 10 seconds, and try again\n",
        "    # if this still fails, we'll throw an exception\n",
        "    import time\n",
        "    time.sleep(10)\n",
        "    connection_details_json_str = !bq show --format json --connection {PROJECT_ID}.{LOCATION}.{BQ_DATASET}\n",
        "    connection_details_dict = json.loads(connection_details_json_str[0])\n",
        "    CONNECTION_SA_ID = connection_details_dict[\"cloudResource\"][\"serviceAccountId\"]\n",
        "if not CONNECTION_SA_ID:\n",
        "    raise ValueError(\"No Service Account detected for BQ Connection\")"
      ],
      "metadata": {
        "id": "Oa7oOviHch-s"
      },
      "id": "Oa7oOviHch-s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud storage buckets add-iam-policy-binding 'gs://{BUCKET_NAME}' \\\n",
        "    --member='serviceAccount:{CONNECTION_SA_ID}' \\\n",
        "    --role=roles/storage.objectUser \\\n",
        "    --quiet\n",
        "\n",
        "!gcloud storage buckets add-iam-policy-binding 'gs://{BUCKET_NAME}' \\\n",
        "    --member='serviceAccount:{CONNECTION_SA_ID}' \\\n",
        "    --role=roles/storage.legacyBucketReader \\\n",
        "    --quiet"
      ],
      "metadata": {
        "id": "2iDcfnAVckUb"
      },
      "id": "2iDcfnAVckUb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Tables in BigQuery"
      ],
      "metadata": {
        "id": "rWQeMCp95S_M"
      },
      "id": "rWQeMCp95S_M"
    },
    {
      "cell_type": "code",
      "source": [
        "bus_stops_uri = f\"gs://{BUCKET}/iceberg_data/bus_stations/\"\n",
        "bus_lines_uri = f\"gs://{BUCKET}/iceberg_data/bus_lines/\"\n",
        "ridership_uri = f\"gs://{BUCKET}/iceberg_data/ridership/\"\n",
        "\n",
        "bigquery_client.query(f\"DROP TABLE IF EXISTS {BQ_DATASET}.bus_stations;\").result()\n",
        "query = f\"\"\"\n",
        "CREATE TABLE {BQ_DATASET}.bus_stations\n",
        "(\n",
        "  bus_stop_id INTEGER,\n",
        "  address STRING,\n",
        "  school_zone BOOLEAN,\n",
        "  seating BOOLEAN,\n",
        "  latitude FLOAT64,\n",
        "  longtitude FLOAT64\n",
        ")\n",
        "WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_DATASET}`\n",
        "OPTIONS (\n",
        "  file_format = 'PARQUET',\n",
        "  table_format = 'ICEBERG',\n",
        "  storage_uri = '{bus_stops_uri}');\n",
        "\"\"\"\n",
        "bigquery_client.query(query).result()"
      ],
      "metadata": {
        "id": "CzJkmvmddWcP"
      },
      "id": "CzJkmvmddWcP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigquery_client.query(f\"DROP TABLE IF EXISTS {BQ_DATASET}.bus_lines;\").result()\n",
        "query = f\"\"\"\n",
        "CREATE TABLE {BQ_DATASET}.bus_lines\n",
        "(\n",
        "  bus_line_id INTEGER,\n",
        "  bus_line STRING,\n",
        "  number_of_stops INTEGER,\n",
        "  stops ARRAY<INTEGER>\n",
        ")\n",
        "WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_DATASET}`\n",
        "OPTIONS (\n",
        "  file_format = 'PARQUET',\n",
        "  table_format = 'ICEBERG',\n",
        "  storage_uri = '{bus_lines_uri}');\n",
        "\"\"\"\n",
        "bigquery_client.query(query).result()"
      ],
      "metadata": {
        "id": "iPwMwFL4-QOQ"
      },
      "id": "iPwMwFL4-QOQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigquery_client.query(f\"DROP TABLE IF EXISTS {BQ_DATASET}.ridership;\").result()\n",
        "query = f\"\"\"\n",
        "CREATE TABLE {BQ_DATASET}.ridership\n",
        "(\n",
        "  transit_timestamp TIMESTAMP,\n",
        "  station_id INTEGER,\n",
        "  ridership INTEGER\n",
        ")\n",
        "WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_DATASET}`\n",
        "OPTIONS (\n",
        "  file_format = 'PARQUET',\n",
        "  table_format = 'ICEBERG',\n",
        "  storage_uri = '{ridership_uri}');\n",
        "\"\"\"\n",
        "bigquery_client.query(query).result()"
      ],
      "metadata": {
        "id": "5nqPhNO5-SKK"
      },
      "id": "5nqPhNO5-SKK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data to Lakehouse tables\n"
      ],
      "metadata": {
        "id": "PyHGPPThddRV"
      },
      "id": "PyHGPPThddRV"
    },
    {
      "cell_type": "code",
      "source": [
        "table_ref = dataset_ref.table(\"bus_lines\")\n",
        "\n",
        "# BQ tables for Apache Iceberg do not support load with truncating, so we will truncate manually, and then load\n",
        "truncate = bigquery_client.query(f\"DELETE FROM {BQ_DATASET}.bus_lines WHERE TRUE\")\n",
        "truncate.result()\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
        "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
        ")\n",
        "\n",
        "job = bigquery_client.load_table_from_uri(\n",
        "    f\"gs://{SOURCE_DATA_BUCKET}/mta_staging_data/bus_lines.json\",\n",
        "    table_ref,\n",
        "    job_config=job_config,\n",
        ")\n",
        "\n",
        "job.result()"
      ],
      "metadata": {
        "id": "v2vn3VeldfPQ"
      },
      "id": "v2vn3VeldfPQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_ref = dataset_ref.table(\"bus_stations\")\n",
        "\n",
        "# BQ tables for Apache Iceberg do not support load with truncating, so we will truncate manually, and then load\n",
        "truncate = bigquery_client.query(f\"DELETE FROM {BQ_DATASET}.bus_stations WHERE TRUE\")\n",
        "truncate.result()\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        ")\n",
        "\n",
        "job = bigquery_client.load_table_from_uri(\n",
        "    f\"gs://{SOURCE_DATA_BUCKET}/mta_staging_data/bus_stations.csv\",\n",
        "    table_ref,\n",
        "    job_config=job_config,\n",
        ")\n",
        "\n",
        "job.result()"
      ],
      "metadata": {
        "id": "UPNZJPUzdgh0"
      },
      "id": "UPNZJPUzdgh0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_ref = dataset_ref.table(\"ridership\")\n",
        "\n",
        "# BQ tables for Apache Iceberg do not support load with truncating, so we will truncate manually, and then load\n",
        "truncate = bigquery_client.query(f\"DELETE FROM {BQ_DATASET}.ridership WHERE TRUE\")\n",
        "truncate.result()\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,\n",
        ")\n",
        "\n",
        "job = bigquery_client.load_table_from_uri(\n",
        "    f\"gs://{SOURCE_DATA_BUCKET}/mta_staging_data/ridership/*.csv\",\n",
        "    table_ref,\n",
        "    job_config=job_config,\n",
        ")\n",
        "\n",
        "job.result()"
      ],
      "metadata": {
        "id": "Wp31JMyK8bkC"
      },
      "id": "Wp31JMyK8bkC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Analytics\n",
        "After loading the data to our open data lakehouse, we will demonstrate some basic analytics, but we will repeat the process with several different engines\n",
        "- BigQuery\n",
        "- Spark (serverless?)\n",
        "- Dataflow"
      ],
      "metadata": {
        "id": "jJCIa_zu5p7P"
      },
      "id": "jJCIa_zu5p7P"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "lakehouse-part1-load-data",
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}