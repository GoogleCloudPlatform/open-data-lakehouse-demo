{"cells": [{"cell_type": "code", "execution_count": null, "id": "vblA", "metadata": {}, "outputs": [], "source": ["import os\n", "LOCATION = os.environ.get(\"LOCATION\", \"us-central1\") \n", "USER_AGENT = \"cloud-solutions/data-to-ai-nb-v3\"\n", "BQ_DATASET = \"ridership_lakehouse\"\n", "\n", "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n", "if not PROJECT_ID:\n", "    import subprocess\n", "    PROJECT_ID = subprocess.run([\"gcloud\", \"config\", \"get-value\", \"project\"], capture_output=True)\n", "    PROJECT_ID = PROJECT_ID.stdout.decode(\"utf-8\").strip()\n", "assert PROJECT_ID, \"Please set the PROJECT_ID environment variable\"\n", "BUCKET_NAME = f\"{PROJECT_ID}-ridership-lakehouse\""]}, {"cell_type": "code", "execution_count": null, "id": "lEQa", "metadata": {}, "outputs": [], "source": ["from google.cloud import storage, bigquery\n", "from google.api_core import exceptions\n", "from google.api_core.client_info import ClientInfo\n", "from google.cloud.exceptions import NotFound\n", "\n", "bigquery_client = bigquery.Client(\n", "    project=PROJECT_ID,\n", "    location=LOCATION,\n", "    client_info=ClientInfo(user_agent=USER_AGENT)\n", ")\n", "storage_client = storage.Client(\n", "    project=PROJECT_ID,\n", "    client_info=ClientInfo(user_agent=USER_AGENT)\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "Xref", "metadata": {}, "outputs": [], "source": ["bus_stops_uri = f\"gs://{BUCKET_NAME}/iceberg_data/bus_stations/\"\n", "bus_lines_uri = f\"gs://{BUCKET_NAME}/iceberg_data/bus_lines/\"\n", "ridership_uri = f\"gs://{BUCKET_NAME}/iceberg_data/ridership/\"\n", "\n", "bigquery_client.query(f\"DROP TABLE IF EXISTS {BQ_DATASET}.bus_stations;\").result()\n", "query = f\"\"\"\n", "CREATE TABLE {BQ_DATASET}.bus_stations\n", "(\n", "  bus_stop_id INTEGER,\n", "  address STRING,\n", "  school_zone BOOLEAN,\n", "  seating BOOLEAN,\n", "  latitude FLOAT64,\n", "  longtitude FLOAT64\n", ")\n", "WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_DATASET}`\n", "OPTIONS (\n", "  file_format = 'PARQUET',\n", "  table_format = 'ICEBERG',\n", "  storage_uri = '{bus_stops_uri}');\n", "\"\"\"\n", "bigquery_client.query(query).result()"]}, {"cell_type": "code", "execution_count": null, "id": "SFPL", "metadata": {}, "outputs": [], "source": ["bigquery_client.query(\n", "    f'DROP TABLE IF EXISTS {BQ_DATASET}.bus_lines;'\n", ").result()\n", "_create_table_stmt = f\"\"\"\n", "    CREATE TABLE {BQ_DATASET}.bus_lines (\n", "        bus_line_id INTEGER,\n", "        bus_line STRING,\n", "        number_of_stops INTEGER,\n", "        stops ARRAY<INTEGER>\n", "    )\n", "    WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_DATASET}`\n", "    OPTIONS (\n", "        file_format = 'PARQUET',\n", "        table_format = 'ICEBERG',\n", "        storage_uri = '{bus_lines_uri}'\n", "    );\n", "\"\"\"\n", "bigquery_client.query(_create_table_stmt).result()"]}, {"cell_type": "code", "execution_count": null, "id": "BYtC", "metadata": {}, "outputs": [], "source": ["bigquery_client.query(\n", "    f'DROP TABLE IF EXISTS {BQ_DATASET}.ridership;'\n", ").result()\n", "_create_table_stmt = f\"\"\"\n", "    CREATE TABLE {BQ_DATASET}.ridership (\n", "        transit_timestamp TIMESTAMP,\n", "        station_id INTEGER,\n", "        ridership INTEGER\n", "    )\n", "    WITH CONNECTION `{PROJECT_ID}.{LOCATION}.{BQ_DATASET}`\n", "    OPTIONS (\n", "        file_format = 'PARQUET',\n", "        table_format = 'ICEBERG',\n", "        storage_uri = '{ridership_uri}'\n", "    );\n", "\"\"\"\n", "bigquery_client.query(_create_table_stmt).result()"]}, {"cell_type": "code", "execution_count": null, "id": "Kclp", "metadata": {}, "outputs": [], "source": ["table_ref = dataset_ref.table(\"bus_lines\")\n", "\n", "# BQ tables for Apache Iceberg do not support load with truncating, so we will truncate manually, and then load\n", "truncate = bigquery_client.query(f\"DELETE FROM {BQ_DATASET}.bus_lines WHERE TRUE\")\n", "truncate.result()\n", "\n", "job_config = bigquery.LoadJobConfig(\n", "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n", "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n", ")\n", "\n", "job = bigquery_client.load_table_from_uri(\n", "    f\"gs://{SOURCE_DATA_BUCKET}/mta_staging_data/bus_lines.json\",\n", "    table_ref,\n", "    job_config=job_config,\n", ")\n", "\n", "job.result()"]}, {"cell_type": "code", "execution_count": null, "id": "emfo", "metadata": {}, "outputs": [], "source": ["table_ref_1 = dataset_ref.table('bus_stations')\n", "truncate_1 = bigquery_client.query(f'DELETE FROM {BQ_DATASET}.bus_stations WHERE TRUE')\n", "truncate_1.result()\n", "job_config_1 = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_APPEND, source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1)\n", "job_1 = bigquery_client.load_table_from_uri(f'gs://{SOURCE_DATA_BUCKET}/mta_staging_data/bus_stations.csv', table_ref_1, job_config=job_config_1)\n", "job_1.result()"]}, {"cell_type": "code", "execution_count": null, "id": "Hstk", "metadata": {}, "outputs": [], "source": ["table_ref_2 = dataset_ref.table('ridership')\n", "truncate_2 = bigquery_client.query(f'DELETE FROM {BQ_DATASET}.ridership WHERE TRUE')\n", "truncate_2.result()\n", "job_config_2 = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_APPEND, source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1)\n", "job_2 = bigquery_client.load_table_from_uri(f'gs://{SOURCE_DATA_BUCKET}/mta_staging_data/ridership/*.csv', table_ref_2, job_config=job_config_2)\n", "job_2.result()"]}, {"cell_type": "code", "execution_count": null, "id": "iLit", "metadata": {}, "outputs": [], "source": ["import marimo as mo"]}, {"cell_type": "markdown", "id": "Hbol", "metadata": {}, "source": ["# Ridership Open Lakehouse Demo\n", "\n", "This notebook will demonstrate a strategy to implement an open lakehouse on GCP, using Apache Iceberg,\n", "as an open source standard for managing data, while still leveraging GCP native capabilities.\n", "\n", "This demo will use BigQuery Manged Iceberg Tables, Managed Apache Kafka and Apache Kafka Connect to ingest\n", "streaming data, Vertex AI for Generative AI queries on top of the data and Dataplex to govern tables.\n", "\n", "This notebook will load the data generated in the previous notebook to BQ, and setup streaming resources"]}, {"cell_type": "markdown", "id": "MJUe", "metadata": {"marimo": {"config": {"hide_code": true}}}, "source": ["## Setup the environment"]}, {"cell_type": "markdown", "id": "bkHC", "metadata": {"marimo": {"config": {"hide_code": true}}}, "source": ["## Create Clients"]}, {"cell_type": "markdown", "id": "PKri", "metadata": {"marimo": {"config": {"hide_code": true}}}, "source": ["## Create Tables in BigQuery"]}, {"cell_type": "markdown", "id": "RGSE", "metadata": {"marimo": {"config": {"hide_code": true}}}, "source": ["## Load data to Lakehouse tables"]}, {"cell_type": "markdown", "id": "nWHF", "metadata": {"marimo": {"config": {"hide_code": true}}}, "source": ["## Basic Analytics\n", "After loading the data to our open data lakehouse, we will demonstrate some basic analytics, but we will repeat the process with several different engines\n", "- BigQuery\n", "- Spark (serverless?)\n", "- Dataflow"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}