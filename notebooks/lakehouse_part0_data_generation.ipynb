{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ridership Open Lakehouse Demo\n",
        "\n",
        "This notebook will demonstrate a strategy to implement an open lakehouse on GCP, using Apache Iceberg, as an open source standard for managing data, while still levergin GCP native capabilities. This demo will use BigQuery Manged Iceberg Tables, Managed Apache Kafka and Apache Kafka Connect to ingest streaming data, Vertex AI for Generative AI queries on top of the data and Dataplex to govern tables.\n",
        "\n",
        "This notebook will generate fake data and anonimized real-world data.\n",
        "\n",
        "the real-world data used in this notebook is from [MTA daily ridership data](https://data.ny.gov/Transportation/MTA-Daily-Ridership-Data-2020-2025/vxuj-8kew/data_preview).\n",
        "\n",
        "\n",
        "Rest of the data is being randomly generated inside the notebook."
      ],
      "metadata": {
        "id": "KCidv5zhhwxp"
      },
      "id": "KCidv5zhhwxp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the environment"
      ],
      "metadata": {
        "id": "rf9BQ43XkJ-4"
      },
      "id": "rf9BQ43XkJ-4"
    },
    {
      "cell_type": "code",
      "id": "VRI2eXitYpbhyydqBweIPeh4",
      "metadata": {
        "tags": [],
        "id": "VRI2eXitYpbhyydqBweIPeh4"
      },
      "source": [
        "!pip install faker pandas sodapy --upgrade --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"your project ID here\" # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "# in-case someone didn't update the project manually, assume current project is the right one\n",
        "if PROJECT_ID == \"your project ID here\":\n",
        "    PROJECT_ID = !gcloud config get-value project\n",
        "    PROJECT_ID = PROJECT_ID[0]\n",
        "\n",
        "BUCKET_NAME = f\"{PROJECT_ID}-ridership-lakehouse\"  # Bucket created in subsequent step\n",
        "USER_AGENT = \"cloud-solutions/data-to-ai-nb-v3\"\n",
        "BQ_DATASET = \"ridership_lakehouse_staging\"\n",
        "PROJECT_ID"
      ],
      "metadata": {
        "id": "fSNkMQ_kdqkp"
      },
      "id": "fSNkMQ_kdqkp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery, storage\n",
        "# @title Create clients for gcs and bq\n",
        "from google.api_core.client_info import ClientInfo\n",
        "\n",
        "bigquery_client = bigquery.Client(\n",
        "    project=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        "    client_info=ClientInfo(user_agent=USER_AGENT)\n",
        ")\n",
        "storage_client = storage.Client(\n",
        "    project=PROJECT_ID,\n",
        "    client_info=ClientInfo(user_agent=USER_AGENT)\n",
        ")\n"
      ],
      "metadata": {
        "id": "8U-sz3qXGoaV"
      },
      "id": "8U-sz3qXGoaV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create GCS bucket, or reference the existing one\n",
        "from google.cloud import exceptions\n",
        "\n",
        "try:\n",
        "    bucket = storage_client.create_bucket(BUCKET_NAME, location=LOCATION)\n",
        "    print(f\"Bucket {BUCKET_NAME} created\")\n",
        "except exceptions.Conflict:\n",
        "    # Bucket already exists - return the existing bucket\n",
        "    bucket = storage_client.bucket(BUCKET_NAME)\n",
        "    print(f\"Bucket {BUCKET_NAME} already exists\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating bucket {BUCKET_NAME}: {e}\")"
      ],
      "metadata": {
        "id": "9X8n8Y1nCclX"
      },
      "id": "9X8n8Y1nCclX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MTA Data\n",
        "\n",
        "This part is tricky - this is the raw data from the MTA subways of new york.\n",
        "The MTA website and API are slow... very slow. Checking the [hourly ridership data](https://data.ny.gov/Transportation/MTA-Subway-Hourly-Ridership-2020-2024/wujg-7c2s/about_data), we have about 110 million records to get.\n",
        "\n",
        "Downloading the CSV manually is the more efficiant option, but still very slow, so you would have to send the request, and keep your machine awake and browser for a few hours (yes, hours, was about 2 hours in my case) before the CSV starts downloading.\n",
        "\n",
        "for programatic download using the API, the situation might be worse. the API is prone to timeouts. The default records limit per request is 1,000, and the maximum is 50,000, which means we have to do chunking of API calls, but the latency is increasing expo. when increasing the limit.\n",
        "\n",
        "I've written the function to donwload the data and write each request to be appended to a file, but this ran for 4 hours, and got around 8% of the data, before I gave up.\n",
        "\n",
        "The next cell has the function to download the data using the API, but the call to the function is commented out, since it is very slow to run.\n",
        "\n",
        "the cell after that allows you to fill in the path to GCS, so, whichever method you want to get the data, just make sure, that the variable `MTA_RAW_CSV` points to a valid and accisble path on GCS that holds the MTA hourly ridership data.\n",
        "\n",
        "happy thoughts!"
      ],
      "metadata": {
        "id": "FG67A_JzdFoR"
      },
      "id": "FG67A_JzdFoR"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get raw MTA data (https://data.ny.gov/Transportation/MTA-Subway-Hourly-Ridership-2020-2024/wujg-7c2s/about_data)\n",
        "from csv import DictWriter\n",
        "from sodapy import Socrata\n",
        "\n",
        "FORCE_CLEAR_DATA = False\n",
        "FILENAME=\"raw-mta-data.csv\"\n",
        "\n",
        "fieldnames = [\n",
        "    'transit_timestamp',\n",
        "    'transit_mode',\n",
        "    'station_complex_id',\n",
        "    'station_complex',\n",
        "    'borough',\n",
        "    'payment_method',\n",
        "    'fare_class_category',\n",
        "    'ridership',\n",
        "    'transfers',\n",
        "    'latitude',\n",
        "    'longitude',\n",
        "    'georeference',\n",
        "    ':@computed_region_kjdx_g34t',\n",
        "    ':@computed_region_yamh_8v7k',\n",
        "    ':@computed_region_wbg7_3whc'\n",
        "]\n",
        "def programmatically_download_mta_data():\n",
        "  import requests\n",
        "  client = Socrata(\"data.ny.gov\", None)\n",
        "\n",
        "  TOTAL_NUMBER_OF_RECORDS = 110_696_370\n",
        "  STEP = 50000\n",
        "\n",
        "  existing_file = os.path.exists(FILENAME)\n",
        "  if not existing_file or FORCE_CLEAR_DATA:\n",
        "    rows_got = 0\n",
        "    with open(FILENAME, \"w\") as fp:\n",
        "      writer = DictWriter(fp, fieldnames=fieldnames)\n",
        "      writer.writeheader()\n",
        "  else:\n",
        "    with open(FILENAME, 'r') as f:\n",
        "      rows_got = sum(1 for line in f) - 1\n",
        "    print(f\"Starting from existing data. already got {rows_got:,} records ({round(rows_got / TOTAL_NUMBER_OF_RECORDS * 100, 2)}%)\")\n",
        "\n",
        "  while rows_got < TOTAL_NUMBER_OF_RECORDS:\n",
        "    try:\n",
        "      results = client.get(\"wujg-7c2s\", limit=STEP, offset=rows_got, )\n",
        "    except requests.exceptions.ReadTimeout as te:\n",
        "      STEP -= 1000\n",
        "      print(f\"Got timeout, adjusting limit to {STEP}\")\n",
        "    else:\n",
        "      with open(FILENAME, \"a\") as fp:\n",
        "        writer = DictWriter(fp, fieldnames=fieldnames)\n",
        "        writer.writerows(results)\n",
        "      rows_got += len(results)\n",
        "      print(f\"Got {rows_got:,} rows so far ({round(rows_got / TOTAL_NUMBER_OF_RECORDS * 100, 2)}%)\")\n",
        "\n",
        "# programmatically_download_mta_data()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DxkiJQJnnnsv"
      },
      "id": "DxkiJQJnnnsv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MTA_RAW_CSV_PATH_IN_GCS = \"mta-manual-downloaded-data/MTA_Subway_Hourly_Ridership.csv\"  # @param {type: 'string'}\n",
        "MTA_RAW_CSV = f\"gs://{BUCKET_NAME}/{MTA_RAW_CSV_PATH_IN_GCS}\"\n",
        "\n",
        "blob_exists_output = !gsutil ls {MTA_RAW_CSV}\n",
        "\n",
        "if blob_exists_output[0].startswith(\"CommandException\"):\n",
        "  raise ValueError(f\"Path '{MTA_RAW_CSV}' doesn't appear to point to a valid GCS object\")\n",
        "else:\n",
        "  print(f\"Path '{MTA_RAW_CSV}' found\")\n"
      ],
      "metadata": {
        "id": "0B0HpC9eIHOr"
      },
      "id": "0B0HpC9eIHOr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load raw data to a BQ, to continue transformation of the data\n",
        "# Due to it's size, it will be easier to load data to bq and transform it there\n",
        "\n",
        "from google.cloud.bigquery import SchemaField\n",
        "\n",
        "# schema for the original raw file\n",
        "mta_schema = [\n",
        "    # Note that the timestamp col is loaded as string, due to US format, which is not automatically detected by BQ\n",
        "    SchemaField(\"transit_timestamp\", \"STRING\"),\n",
        "    SchemaField(\"transit_mode\", \"STRING\"),\n",
        "    SchemaField(\"station_complex_id\", \"STRING\"),\n",
        "    SchemaField(\"station_complex\", \"STRING\"),\n",
        "    SchemaField(\"borough\", \"STRING\"),\n",
        "    SchemaField(\"payment_method\", \"STRING\"),\n",
        "    SchemaField(\"fare_class_category\", \"STRING\"),\n",
        "    SchemaField(\"ridership\", \"INTEGER\"),\n",
        "    SchemaField(\"transfers\", \"INTEGER\"),\n",
        "    SchemaField(\"latitude\", \"FLOAT\"),\n",
        "    SchemaField(\"longtitude\", \"FLOAT\"),\n",
        "    SchemaField(\"Georeference\", \"GEOGRAPHY\"),\n",
        "    ]\n",
        "\n",
        "BQ_TABLE = \"raw_mta_data\"\n",
        "\n",
        "dataset = bigquery.Dataset(f'{PROJECT_ID}.{BQ_DATASET}')\n",
        "dataset.location = LOCATION\n",
        "\n",
        "# create or get a reference to existing dataset\n",
        "try:\n",
        "  bigquery_client.get_dataset(BQ_DATASET)\n",
        "  print(\"dataset exists\")\n",
        "except exceptions.NotFound:\n",
        "  bigquery_client.create_dataset(dataset, timeout=30)\n",
        "  print('dataset created {}'.format(BQ_DATASET))\n",
        "\n",
        "try:\n",
        "    table_ref = dataset.table(BQ_TABLE)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        schema=mta_schema,\n",
        "    )\n",
        "    load_job = bigquery_client.load_table_from_uri(\n",
        "        MTA_RAW_CSV, table_ref, job_config=job_config\n",
        "    )\n",
        "    load_job.result()\n",
        "\n",
        "    print('created {}.{}'.format(BQ_DATASET, BQ_TABLE))\n",
        "\n",
        "except Exception as e:\n",
        "    print('mta load failed {}'.format(e))\n"
      ],
      "metadata": {
        "id": "_5gTeTDy2CmB"
      },
      "id": "_5gTeTDy2CmB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigquery_client.query(f\"DROP TABLE IF EXISTS {BQ_DATASET}.mta_data_stations;\").result()\n",
        "\n",
        "# the raw data has a non-normalized dataset, where each station appear in full detail, for each hourly data point\n",
        "# we will create a table just for the stations, and we will reference the `station_id` from the thinner time-series table.\n",
        "# note, we are saving the results to a pandas dataframe, to be used later\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE TABLE {BQ_DATASET}.mta_data_stations AS\n",
        "SELECT\n",
        "  CAST(REPLACE(station_complex_id, 'TRAM', '98765') AS INT64) AS station_id,\n",
        "  station_complex,\n",
        "  borough,\n",
        "  latitude,\n",
        "  longtitude,\n",
        "FROM\n",
        "  (\n",
        "    SELECT\n",
        "      *,\n",
        "      ROW_NUMBER() OVER (PARTITION BY station_complex_id ORDER BY transit_timestamp ASC) as rn\n",
        "    FROM\n",
        "      `{BQ_DATASET}.raw_mta_data`\n",
        "  )\n",
        "WHERE\n",
        "  rn = 1;\n",
        "\"\"\"\n",
        "bigquery_client.query(query).result()\n",
        "\n",
        "stations_df = bigquery_client.query(f\"SELECT * FROM {BQ_DATASET}.mta_data_stations;\").to_dataframe()\n",
        "stations_df"
      ],
      "metadata": {
        "id": "oI8t74fPPH2n"
      },
      "id": "oI8t74fPPH2n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigquery_client.query(f\"DROP TABLE IF EXISTS {BQ_DATASET}.mta_data_parsed;\").result()\n",
        "\n",
        "# this query will generate a table of the hourly data, where each station ID and timestamp, are grouped, and the ridership is summed up.\n",
        "# the original table breaks down ridership by method of payment, fare class and other attributes, which are not of interest to us\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE TABLE `{BQ_DATASET}.mta_data_parsed` AS\n",
        "SELECT\n",
        "  PARSE_TIMESTAMP('%m/%d/%Y %I:%M:%S %p', transit_timestamp) AS `transit_timestamp`,\n",
        "  CAST(REPLACE(station_complex_id, 'TRAM', '98765') AS INT64) AS `station_id`,\n",
        "  SUM(ridership) as ridership\n",
        "FROM `{BQ_DATASET}.raw_mta_data`\n",
        "GROUP BY PARSE_TIMESTAMP('%m/%d/%Y %I:%M:%S %p', transit_timestamp), CAST(REPLACE(station_complex_id, 'TRAM', '98765') AS INT64);\n",
        "\"\"\"\n",
        "bigquery_client.query(query).result()\n",
        "bigquery_client.query(f\"SELECT * FROM {BQ_DATASET}.mta_data_parsed LIMIT 20;\").to_dataframe()"
      ],
      "metadata": {
        "id": "iP2p0oNhOAGd"
      },
      "id": "iP2p0oNhOAGd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigquery_client.query(f\"DROP TABLE IF EXISTS {BQ_DATASET}.ridership;\").result()\n",
        "\n",
        "# create a table with data minute-by-minute instead of hourly data, distributing the data\n",
        "# this means that if a data point in the raw data mentioned 30 riders for 8am,\n",
        "# the new table would have 30 data points (between 8am to 8:30am) with 1 rider each\n",
        "# and another 30 data points with 0 riders\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE TABLE `{BQ_DATASET}.ridership` AS\n",
        "SELECT\n",
        "  TIMESTAMP_ADD(t.transit_timestamp, INTERVAL minute_offset MINUTE) AS transit_timestamp,\n",
        "  t.station_id,\n",
        "  ROUND(CAST(\n",
        "    (FLOOR(t.ridership / 60)) +\n",
        "    CASE\n",
        "      WHEN minute_offset < MOD(t.ridership, 60) THEN 1\n",
        "      ELSE 0\n",
        "    END AS INTEGER\n",
        "  )) AS ridership\n",
        "FROM\n",
        "  {BQ_DATASET}.mta_data_parsed AS t,\n",
        "  UNNEST(GENERATE_ARRAY(0, 59)) AS minute_offset\n",
        "ORDER BY\n",
        "  station_id,\n",
        "  transit_timestamp;\n",
        "\"\"\"\n",
        "bigquery_client.query(query).result()\n",
        "bigquery_client.query(f\"SELECT * FROM {BQ_DATASET}.ridership LIMIT 20;\").to_dataframe()"
      ],
      "metadata": {
        "id": "fTcaYrqFc8t8"
      },
      "id": "fTcaYrqFc8t8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# re-aggregate the minute-by-minute data and compare to the original hourly data to verify correctness of our distributed minutly data\n",
        "# it should return no results, as the `WHERE` clause mentions to return rows only where the original data point, and the aggregated data have a mismatch\n",
        "\n",
        "query = f\"\"\"\n",
        "WITH minutly_agg AS (\n",
        "  SELECT TIMESTAMP_TRUNC(transit_timestamp, hour) AS transit_timestamp,\n",
        "  station_id,\n",
        "  SUM(ridership) AS ridership_agg\n",
        "  FROM `{BQ_DATASET}.ridership`\n",
        "  GROUP BY TIMESTAMP_TRUNC(transit_timestamp, hour),\n",
        "  station_id\n",
        ")\n",
        "SELECT\n",
        "  minutly_agg.transit_timestamp, minutly_agg.station_id, minutly_agg.ridership_agg, parsed.ridership\n",
        "FROM minutly_agg JOIN `{BQ_DATASET}.mta_data_parsed` as parsed ON\n",
        "  minutly_agg.transit_timestamp = parsed.transit_timestamp AND\n",
        "  minutly_agg.station_id = parsed.station_id\n",
        "WHERE minutly_agg.ridership_agg != parsed.ridership\n",
        "\"\"\"\n",
        "bigquery_client.query(query).to_dataframe()"
      ],
      "metadata": {
        "id": "hFp6-0OSDn0K"
      },
      "id": "hFp6-0OSDn0K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate fake data for bus lines and bus stops (Routes)\n",
        "\n",
        "from faker import Faker\n",
        "Faker.seed(42)\n",
        "fake = Faker()\n",
        "\n",
        "import random\n",
        "def generate_bus_line(i: int) -> dict:\n",
        "  number_of_stops = fake.random_int(min=40, max=60)\n",
        "  return {\n",
        "      \"bus_line_id\": i+1,\n",
        "      \"bus_line\": fake.unique.bothify(\"?-###\").upper(),\n",
        "      \"number_of_stops\": number_of_stops,\n",
        "      \"stops\": [random.choice(stations_ids) for _ in range(number_of_stops)]\n",
        "  }\n",
        "\n",
        "def fakify_station(station: dict) -> dict:\n",
        "  return {\n",
        "      \"bus_stop_id\": station[\"station_id\"],\n",
        "      \"address\": fake.unique.address().split(\",\")[0].replace(\"\\n\", \", \"),\n",
        "      \"school_zone\": fake.boolean(),\n",
        "      \"seating\": fake.boolean(),\n",
        "      \"latitude\": station[\"latitude\"],\n",
        "      \"longtitude\": station[\"longtitude\"],\n",
        "  }\n",
        "\n",
        "stations_lst = stations_df.to_dict('records')\n",
        "fake_stations_lst = [fakify_station(station) for station in stations_lst]\n",
        "stations_ids = [station[\"bus_stop_id\"] for station in fake_stations_lst]\n",
        "BUS_LINES_NUM = 25\n",
        "bus_lines = [generate_bus_line(i) for i in range(BUS_LINES_NUM)]\n",
        "\n",
        "random_bus_line = random.choice(bus_lines)\n",
        "print(f\"Genrated {len(bus_lines)} random bus lines. One for example: {random_bus_line}\")\n",
        "print(f\"Anonimized {len(fake_stations_lst)} bus_stations. The first stations for the random bus line above is: {next(filter(lambda x: x['bus_stop_id'] == random_bus_line['stops'][0], fake_stations_lst))}\")"
      ],
      "metadata": {
        "id": "WPQlp1-j7V5b"
      },
      "id": "WPQlp1-j7V5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract data to GCS\n",
        "Now, that we have all data we need, the data needs to reside in GCS. We will extract the `ridership` BigQuery table to GCS as a CSV, we will store the local datasets (`bus_lines` & `fake_stations_lst`) we will save to local files (JSONL and CSV, respectively) and upload to GCS."
      ],
      "metadata": {
        "id": "3372qZJuYZAj"
      },
      "id": "3372qZJuYZAj"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Extract ridership data to GCS\n",
        "# Note this dataset is quite big, so we have to export it as parted CSV\n",
        "destination_uri = \"gs://{}/{}\".format(BUCKET_NAME, \"mta_staging_data/ridership/ridership_part_*.csv\")\n",
        "table_ref = dataset.table(\"ridership\")\n",
        "\n",
        "extract_job = bigquery_client.extract_table(\n",
        "    table_ref,\n",
        "    destination_uri,\n",
        "    # Location must match that of the source table.\n",
        "    location=LOCATION,\n",
        ")  # API request\n",
        "extract_job.result()  # Waits for job to complete."
      ],
      "metadata": {
        "id": "iJn2pDX_XDiF"
      },
      "id": "iJn2pDX_XDiF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"bus_stations.csv\", \"w\") as fp:\n",
        "  writer = DictWriter(fp, fieldnames=fake_stations_lst[0].keys())\n",
        "  writer.writeheader()\n",
        "  writer.writerows(fake_stations_lst)\n",
        "\n",
        "import json\n",
        "with open(\"bus_lines.json\", \"w\") as fp:\n",
        "  for line in bus_lines:\n",
        "    json.dump(line, fp)\n",
        "    fp.write(\"\\n\")"
      ],
      "metadata": {
        "id": "2yu9PQVsbeUz"
      },
      "id": "2yu9PQVsbeUz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp bus_lines.json gs://{BUCKET_NAME}/mta_staging_data/bus_lines.json\n",
        "!gsutil cp bus_stations.csv gs://{BUCKET_NAME}/mta_staging_data/bus_stations.csv"
      ],
      "metadata": {
        "id": "mWuzZDChb_NF"
      },
      "id": "mWuzZDChb_NF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title Drop tables and dataset, just to keep things clean\n",
        "# try:\n",
        "#   for table in bigquery_client.list_tables(dataset):\n",
        "#     bigquery_client.delete_table(table)\n",
        "#   bigquery_client.delete_dataset(dataset)\n",
        "# except exceptions.NotFound:\n",
        "#   print(\"Dataset looks already dropped\")\n"
      ],
      "metadata": {
        "id": "P4e8NULUAZgw"
      },
      "id": "P4e8NULUAZgw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5q8onGRiA3z2"
      },
      "id": "5q8onGRiA3z2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "lakehouse-part0-data-generation"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}