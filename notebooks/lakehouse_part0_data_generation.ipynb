{"cells": [{"cell_type": "markdown", "id": "MJUe", "metadata": {"marimo": {"config": {"hide_code": true}}, "id": "MJUe"}, "source": ["# Ridership Open Lakehouse Demo (Part 0): Generating our datasets\n", "\n", "This notebook will demonstrate a strategy to implement an open lakehouse on GCP, using Apache Iceberg,\n", "as an open source standard for managing data, while still leveraging GCP native capabilities. This demo will use\n", "BigQuery Manged Iceberg Tables, Managed Apache Kafka and Apache Kafka Connect to ingest streaming data, Vertex AI for Generative AI queries on top of the data and Dataplex to govern tables.\n", "\n", "This notebook will generate fake data and anonimized real-world data.\n", "\n", "the real-world data used in this notebook is from [MTA daily ridership data](https://data.ny.gov/Transportation/MTA-Daily-Ridership-Data-2020-2025/vxuj-8kew/data_preview).\n", "\n", "Rest of the data is being randomly generated inside the notebook."], "execution_count": null}, {"cell_type": "markdown", "id": "vblA", "metadata": {"marimo": {"config": {"hide_code": true}}, "id": "vblA"}, "source": ["## Setup the environment"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "bkHC", "metadata": {"id": "bkHC", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "1fc3e414-d014-45ee-ce94-18f65deb3419"}, "outputs": [], "source": ["import os\n", "USER_AGENT = \"cloud-solutions/data-to-ai-nb-v3\"\n", "\n", "PROJECT_ID = !gcloud config get-value project\n", "PROJECT_ID = PROJECT_ID[0]\n", "STAGING_BQ_DATASET = \"ridership_lakehouse_staging\"\n", "BUCKET_NAME = f\"{PROJECT_ID}-ridership-lakehouse\"\n", "LOCATION = \"us-central1\"\n", "\n", "print(PROJECT_ID)\n", "print(BUCKET_NAME)"]}, {"cell_type": "code", "source": ["!pip install sodapy faker --quiet"], "metadata": {"id": "OA7d79AKI5O9", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "015795c3-bef3-4fb0-dc21-51cf153ce862"}, "id": "OA7d79AKI5O9", "execution_count": null, "outputs": []}, {"cell_type": "code", "execution_count": null, "id": "lEQa", "metadata": {"id": "lEQa"}, "outputs": [], "source": ["from google.cloud import bigquery, storage\n", "from google.api_core.client_info import ClientInfo\n", "\n", "bigquery_client = bigquery.Client(\n", "    project=PROJECT_ID,\n", "    location=LOCATION,\n", "    client_info=ClientInfo(user_agent=USER_AGENT)\n", ")\n", "storage_client = storage.Client(\n", "    project=PROJECT_ID,\n", "    client_info=ClientInfo(user_agent=USER_AGENT)\n", ")"]}, {"cell_type": "markdown", "id": "PKri", "metadata": {"marimo": {"config": {"hide_code": true}}, "id": "PKri"}, "source": ["## MTA Data - PLEASE READ!\n", "\n", "This part is tricky - this is the raw data from the MTA subways of new york.\n", "The MTA website and API are slow... very slow. Checking the [hourly ridership data](https://data.ny.gov/Transportation/MTA-Subway-Hourly-Ridership-2020-2024/wujg-7c2s/about_data), we have about 110 million records to get.\n", "\n", "Downloading the CSV manually is the more efficiant option, but still very slow, so you would have to send the request, and keep your machine awake and browser for a few hours (yes, hours, was about 2 hours in my case) before the CSV starts downloading.\n", "\n", "for programatic download using the API, the situation might be worse. the API is prone to timeouts. The default records limit per request is 1,000, and the maximum is 50,000, which means we have to do chunking of API calls, but the latency is increasing expo. when increasing the limit.\n", "\n", "I've written the function to donwload the data and write each request to be appended to a file, but this ran for 4 hours, and got around 8% of the data, before I gave up.\n", "\n", "The next cell has the function to download the data using the API, but the call to the function is commented out, since it is very slow to run.\n", "\n", "the cell after that allows you to fill in the path to GCS, so, whichever method you want to get the data, just make sure, that the variable `MTA_RAW_CSV` points to a valid and accisble path on GCS that holds the MTA hourly ridership data.\n", "\n", "happy thoughts!"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "Xref", "metadata": {"id": "Xref"}, "outputs": [], "source": ["from csv import DictWriter\n", "from sodapy import Socrata\n", "FILENAME = 'raw-mta-data.csv'\n", "fieldnames = [\n", "    'transit_timestamp',\n", "    'transit_mode',\n", "    'station_complex_id',\n", "    'station_complex',\n", "    'borough',\n", "    'payment_method',\n", "    'fare_class_category',\n", "    'ridership',\n", "    'transfers',\n", "    'latitude',\n", "    'longitude',\n", "    'georeference',\n", "    ':@computed_region_kjdx_g34t',\n", "    ':@computed_region_yamh_8v7k',\n", "    ':@computed_region_wbg7_3whc'\n", "]\n", "\n", "# This function will use the api to download the data into a csv locally\n", "# Note that the final size of the CSV would be around 17GB\n", "# also note that the API is slow and prone to timeout errors\n", "# this is why it has a back-off mechanism where we start with the\n", "# maximum records allowed and back off whenever we have a timeout error\n", "# as mentioned above, a much easier approach would be to go to the website\n", "# and download the CSV manually (although that takes some time as well)\n", "# then upload the CSV to GCS\n", "# This method is here for convenience and is not currently being called.\n", "\n", "# this method was planned to be able to resume from where it last stopped\n", "# we have the option to read an existing CSV that we started downloading\n", "# if you want to ignore the existing file, you can delete it manually or just flip this flag\n", "FORCE_CLEAR_DATA = False\n", "\n", "def programmatically_download_mta_data():\n", "    import requests\n", "    client = Socrata('data.ny.gov', None)\n", "\n", "    # I got this number after downloading the full file and looking at it.\n", "    # Since this should be a static dataset, the number shouldn't change over time\n", "    TOTAL_NUMBER_OF_RECORDS = 110_696_370\n", "    STEP = 50_000\n", "\n", "    # is there is current file already exists\n", "    existing_file = os.path.exists(FILENAME)\n", "\n", "    if not existing_file or FORCE_CLEAR_DATA:\n", "        # if we no current file exists, or flag to ignore it is raised\n", "        rows_got = 0\n", "        with open(FILENAME, 'w') as fp:\n", "            writer = DictWriter(fp, fieldnames=fieldnames)\n", "            # write headers\n", "            writer.writeheader()\n", "    else:\n", "        with open(FILENAME, 'r') as f:\n", "            # read how many records we have already (minus 1 for headers)\n", "            rows_got = sum((1 for line in f)) - 1\n", "        print(f'''Starting from existing data. already got {rows_got:,}\n", "        records ({round(rows_got / TOTAL_NUMBER_OF_RECORDS * 100, 2)}%)''')\n", "\n", "    # while the number of rows we got is smaller then the total number of rows expected\n", "    while rows_got < TOTAL_NUMBER_OF_RECORDS:\n", "        try:\n", "            # get more data\n", "            results = client.get('wujg-7c2s', limit=STEP, offset=rows_got)\n", "        except requests.exceptions.ReadTimeout:\n", "            # in case of a timeout, ask for less data\n", "            STEP = STEP - 1000\n", "            print(f'Got timeout, adjusting limit to {STEP}')\n", "        else:\n", "            # when we get data, append it to the file.\n", "            with open(FILENAME, 'a') as fp:\n", "                writer = DictWriter(fp, fieldnames=fieldnames)\n", "                writer.writerows(results)\n", "            rows_got = rows_got + len(results)\n", "            print(f'Got {rows_got:,} rows so far ({round(rows_got / TOTAL_NUMBER_OF_RECORDS * 100, 2)}%)')\n", "\n", "    # TODO: implement code to upload the local CSV to GCS"]}, {"cell_type": "code", "execution_count": null, "id": "SFPL", "metadata": {"id": "SFPL", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "bd2885c3-8e6c-488d-dd01-5c8219eb8516"}, "outputs": [], "source": ["# This should be a path pointing to the file in GCS.\n", "MTA_RAW_CSV_PATH_IN_GCS = \"mta-manual-downloaded-data/MTA_Subway_Hourly_Ridership.csv\"\n", "MTA_RAW_CSV = f\"gs://{BUCKET_NAME}/{MTA_RAW_CSV_PATH_IN_GCS}\"\n", "\n", "bucket = storage_client.bucket(BUCKET_NAME)\n", "_mta_raw_csv_blob = bucket.get_blob(MTA_RAW_CSV_PATH_IN_GCS)\n", "\n", "if _mta_raw_csv_blob:\n", "    print(f\"Path '{MTA_RAW_CSV}' found\")\n", "else:\n", "    raise ValueError(f\"Path '{MTA_RAW_CSV}' doesn't appear to point to a valid GCS object\")"]}, {"cell_type": "code", "execution_count": null, "id": "BYtC", "metadata": {"id": "BYtC", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "09cd169a-1f4f-4926-aa73-d017375de134"}, "outputs": [], "source": ["# Load raw data to a BQ, to continue transformation of the data\n", "# Due to its size, it will be easier to load data to bq and transform it there\n", "\n", "from google.cloud.bigquery import SchemaField\n", "\n", "# schema for the original raw file\n", "mta_schema = [\n", "    # Note that the timestamp col is loaded as string, due to US format, which is not automatically detected by BQ\n", "    SchemaField(\"transit_timestamp\", \"STRING\"),\n", "    SchemaField(\"transit_mode\", \"STRING\"),\n", "    SchemaField(\"station_complex_id\", \"STRING\"),\n", "    SchemaField(\"station_complex\", \"STRING\"),\n", "    SchemaField(\"borough\", \"STRING\"),\n", "    SchemaField(\"payment_method\", \"STRING\"),\n", "    SchemaField(\"fare_class_category\", \"STRING\"),\n", "    SchemaField(\"ridership\", \"INTEGER\"),\n", "    SchemaField(\"transfers\", \"INTEGER\"),\n", "    SchemaField(\"latitude\", \"FLOAT\"),\n", "    SchemaField(\"longtitude\", \"FLOAT\"),\n", "    SchemaField(\"Georeference\", \"GEOGRAPHY\"),\n", "    ]\n", "\n", "BQ_TABLE = \"raw_mta_data\"\n", "\n", "dataset = bigquery.Dataset(f'{PROJECT_ID}.{STAGING_BQ_DATASET}')\n", "\n", "table_ref = dataset.table(BQ_TABLE)\n", "\n", "job_config = bigquery.LoadJobConfig(\n", "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n", "    source_format=bigquery.SourceFormat.CSV,\n", "    skip_leading_rows=1,\n", "    schema=mta_schema,\n", ")\n", "load_job = bigquery_client.load_table_from_uri(\n", "    MTA_RAW_CSV, table_ref, job_config=job_config\n", ")\n", "load_job.result()\n", "\n", "print('created {}.{}'.format(STAGING_BQ_DATASET, BQ_TABLE))\n"]}, {"cell_type": "markdown", "id": "RGSE", "metadata": {"marimo": {"config": {"hide_code": true}}, "id": "RGSE"}, "source": ["### The `mta_data_stations` table\n", "\n", "the raw data has a non-normalized dataset, where each station appear in full detail, for each hourly data point\n", "we will create a table just for the stations, and we will reference the `station_id` from the thinner time-series table.\n", "\n", "note, we are saving the results to a pandas dataframe, to be used later"], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "Kclp", "metadata": {"id": "Kclp", "colab": {"base_uri": "https://localhost:8080/", "height": 423}, "outputId": "3709d4b0-8f41-4ba7-835c-12e62fbae61c"}, "outputs": [], "source": ["bigquery_client.query(f\"DROP TABLE IF EXISTS {STAGING_BQ_DATASET}.mta_data_stations;\").result()\n", "\n", "_query = f\"\"\"\n", "CREATE TABLE {STAGING_BQ_DATASET}.mta_data_stations AS\n", "SELECT\n", "  CAST(REPLACE(station_complex_id, 'TRAM', '98765') AS INT64) AS station_id,\n", "  station_complex,\n", "  borough,\n", "  latitude,\n", "  longtitude,\n", "FROM\n", "  (\n", "    SELECT\n", "      *,\n", "      ROW_NUMBER() OVER (PARTITION BY station_complex_id ORDER BY transit_timestamp ASC) as rn\n", "    FROM\n", "      `{STAGING_BQ_DATASET}.raw_mta_data`\n", "  )\n", "WHERE\n", "  rn = 1;\n", "\"\"\"\n", "bigquery_client.query(_query).result()\n", "\n", "stations_df = bigquery_client.query(f\"SELECT * FROM {STAGING_BQ_DATASET}.mta_data_stations;\").to_dataframe()\n", "stations_df"]}, {"cell_type": "markdown", "id": "emfo", "metadata": {"marimo": {"config": {"hide_code": true}}, "id": "emfo"}, "source": ["### The `mta_data_parsed` table - temporary table\n", "\n", "This table is a temporary table to hold an hourly data points, with parsed timestamps and station IDs as integers."], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "Hstk", "metadata": {"id": "Hstk", "colab": {"base_uri": "https://localhost:8080/", "height": 676}, "outputId": "178873a6-b84e-419b-8908-2de6a311152a"}, "outputs": [], "source": ["bigquery_client.query(\n", "    f'DROP TABLE IF EXISTS {STAGING_BQ_DATASET}.mta_data_parsed;'\n", ").result()\n", "\n", "_query = f\"\"\"\n", "    CREATE TABLE `{STAGING_BQ_DATASET}.mta_data_parsed` AS\n", "        SELECT\n", "            PARSE_TIMESTAMP('%m/%d/%Y %I:%M:%S %p', transit_timestamp) AS `transit_timestamp`,\n", "            CAST(REPLACE(station_complex_id, 'TRAM', '98765') AS INT64) AS `station_id`,\n", "            SUM(ridership) as ridership\n", "        FROM `{STAGING_BQ_DATASET}.raw_mta_data`\n", "        GROUP BY PARSE_TIMESTAMP('%m/%d/%Y %I:%M:%S %p', transit_timestamp),\n", "        CAST(REPLACE(station_complex_id, 'TRAM', '98765') AS INT64);\n", "\"\"\"\n", "bigquery_client.query(_query).result()\n", "bigquery_client.query(f'SELECT * FROM {STAGING_BQ_DATASET}.mta_data_parsed LIMIT 20;').to_dataframe()"]}, {"cell_type": "markdown", "id": "nWHF", "metadata": {"id": "nWHF"}, "source": ["### The `ridership` table\n", "\n", "This is the output table to hold minute-by-minute data points, spreading each hour evenly between 60 minutes within the hour."], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "iLit", "metadata": {"id": "iLit", "colab": {"base_uri": "https://localhost:8080/", "height": 676}, "outputId": "338557ed-86ff-4fdd-a309-d6046fcae194"}, "outputs": [], "source": ["bigquery_client.query(\n", "    f'DROP TABLE IF EXISTS {STAGING_BQ_DATASET}.ridership;'\n", ").result()\n", "_query = f\"\"\"\n", "    CREATE TABLE `{STAGING_BQ_DATASET}.ridership` AS\n", "    SELECT\n", "        TIMESTAMP_ADD(t.transit_timestamp, INTERVAL minute_offset MINUTE) AS transit_timestamp,\n", "        t.station_id,\n", "        ROUND(CAST(\n", "            (FLOOR(t.ridership / 60)) +\n", "            CASE\n", "                WHEN minute_offset < MOD(t.ridership, 60) THEN 1\n", "                ELSE 0\n", "            END AS INTEGER\n", "        )) AS ridership\n", "    FROM {STAGING_BQ_DATASET}.mta_data_parsed AS t,\n", "    UNNEST(GENERATE_ARRAY(0, 59)) AS minute_offset\n", "    ORDER BY station_id, transit_timestamp;\n", "\"\"\"\n", "bigquery_client.query(_query).result()\n", "bigquery_client.query(f'SELECT * FROM {STAGING_BQ_DATASET}.ridership LIMIT 20;').to_dataframe()"]}, {"cell_type": "code", "execution_count": null, "id": "ZHCJ", "metadata": {"id": "ZHCJ", "colab": {"base_uri": "https://localhost:8080/", "height": 53}, "outputId": "355abaf8-0bed-4e25-947d-73fcc77bf60d"}, "outputs": [], "source": ["# This query is just a verification that the sum of each hour in our minute-by-minute data equals to the data in\n", "# the temporary hourly data\n", "# The query re-aggregates the data by hour, and compars to the original hourly data\n", "# it should return 0 rows, as it filters for hours where the sum of ridership in an hour doesn't equal the original data.\n", "_query = f\"\"\"\n", "    WITH minutly_agg AS (\n", "        SELECT\n", "            TIMESTAMP_TRUNC(transit_timestamp, hour) AS transit_timestamp,\n", "            station_id,\n", "            SUM(ridership) AS ridership_agg\n", "        FROM `{STAGING_BQ_DATASET}.ridership`\n", "        GROUP BY TIMESTAMP_TRUNC(transit_timestamp, hour), station_id\n", "    )\n", "    SELECT\n", "        minutly_agg.transit_timestamp,\n", "        minutly_agg.station_id,\n", "        minutly_agg.ridership_agg,\n", "        parsed.ridership\n", "    FROM minutly_agg\n", "    JOIN `{STAGING_BQ_DATASET}.mta_data_parsed` as parsed\n", "        ON minutly_agg.transit_timestamp = parsed.transit_timestamp AND\n", "            minutly_agg.station_id = parsed.station_id\n", "    WHERE minutly_agg.ridership_agg != parsed.ridership\"\"\"\n", "bigquery_client.query(_query).to_dataframe()"]}, {"cell_type": "markdown", "id": "ROlb", "metadata": {"id": "ROlb"}, "source": ["## Generate fake data for bus lines and bus stops (routes)\n", "\n", "This is based on the stations data in thr original dataset, but the station addresses are faked (using faker),\n", "and then routes are being constructed just by picking randomly from the stations list. We're using `random.sample`\n", "to select a range but without duplicates (as that might create a circular route for a bus, which is not a typical route)\n", "\n", "We will then load the faked data into BigQuery, in order to create time windows of the ridership (that represents people waiting in stations) to simulate bus riders, accumilating riders into a bus driving through their stations."], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "qnkX", "metadata": {"id": "qnkX", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "82d18021-1aa8-44bc-9f60-3b682cdf84fe"}, "outputs": [], "source": ["from faker import Faker\n", "import numpy as np\n", "Faker.seed(42)\n", "fake = Faker()\n", "\n", "import random\n", "from typing import List\n", "def generate_bus_line(i: int, stations_ids: List[int]) -> dict:\n", "  # randomize a number of stops for this line, with the mean of 35, and roughly between 30 and 40\n", "  number_of_stops = int(np.random.normal(loc=35, scale=2))\n", "  return {\n", "      \"bus_line_id\": i+1,\n", "      \"bus_line\": fake.unique.bothify(\"?-###\").upper(),\n", "      \"number_of_stops\": number_of_stops,\n", "      \"stops\": random.sample(stations_ids, k=number_of_stops),\n", "      \"frequency_minutes\": random.choice([5, 10, 15, 20]),\n", "  }\n", "\n", "def fakify_station(station: dict) -> dict:\n", "  return {\n", "      \"bus_stop_id\": station[\"station_id\"],\n", "      \"address\": fake.unique.address().split(\",\")[0].replace(\"\\n\", \", \"),\n", "      \"school_zone\": fake.boolean(),\n", "      \"seating\": fake.boolean(),\n", "      \"borough\": station[\"borough\"],\n", "      \"latitude\": station[\"latitude\"],\n", "      \"longtitude\": station[\"longtitude\"],\n", "  }\n", "\n", "\n", "stations_lst = stations_df.to_dict('records')\n", "fake_stations_lst = [fakify_station(station) for station in stations_lst]\n", "stations_ids = [station[\"bus_stop_id\"] for station in fake_stations_lst]\n", "BUS_LINES_NUM = 25\n", "bus_lines = [generate_bus_line(i, stations_ids) for i in range(BUS_LINES_NUM)]\n", "\n", "random_bus_line = random.choice(bus_lines)\n", "print(f\"Genrated {len(bus_lines)} random bus lines. One for example: {random_bus_line}\")\n", "print(f\"Anonimized {len(fake_stations_lst)} bus_stations. The first stations for the random bus line above is: {next(filter(lambda x: x['bus_stop_id'] == random_bus_line['stops'][0], fake_stations_lst))}\")"]}, {"cell_type": "code", "execution_count": null, "id": "DnEU", "metadata": {"id": "DnEU"}, "outputs": [], "source": ["with open(\"bus_stations.csv\", \"w\") as fp:\n", "  writer = DictWriter(fp, fieldnames=fake_stations_lst[0].keys())\n", "  writer.writeheader()\n", "  writer.writerows(fake_stations_lst)\n", "\n", "import json\n", "with open(\"bus_lines.json\", \"w\") as fp:\n", "  for line in bus_lines:\n", "    json.dump(line, fp)\n", "    fp.write(\"\\n\")"]}, {"cell_type": "code", "source": ["bus_lines_schema = [\n", "    SchemaField(\"bus_line_id\", \"INTEGER\"),\n", "    SchemaField(\"bus_line\", \"STRING\"),\n", "    SchemaField(\"number_of_stops\", \"INTEGER\"),\n", "    SchemaField(\"stops\", \"INTEGER\", mode=\"REPEATED\"),\n", "    SchemaField(\"frequency_minutes\", \"INTEGER\"),\n", "    ]\n", "\n", "BQ_TABLE = \"bus_lines\"\n", "\n", "dataset = bigquery.Dataset(f'{PROJECT_ID}.{STAGING_BQ_DATASET}')\n", "\n", "\n", "table_ref = dataset.table(BQ_TABLE)\n", "\n", "job_config = bigquery.LoadJobConfig(\n", "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n", "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n", "    schema=bus_lines_schema,\n", ")\n", "with open(\"bus_lines.json\", \"rb\") as fp:\n", "  load_job = bigquery_client.load_table_from_file(\n", "      fp, table_ref, job_config=job_config\n", "  )\n", "  load_job.result()\n", "\n", "print('created {}.{}'.format(STAGING_BQ_DATASET, BQ_TABLE))\n"], "metadata": {"id": "6TT9ffTk7WwU", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "c5405273-c38d-4b80-90da-414326799d84"}, "id": "6TT9ffTk7WwU", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["bus_stations_schema = [\n", "    SchemaField(\"bus_stop_id\", \"INTEGER\"),\n", "    SchemaField(\"address\", \"STRING\"),\n", "    SchemaField(\"school_zone\", \"BOOLEAN\"),\n", "    SchemaField(\"seating\", \"BOOLEAN\"),\n", "    SchemaField(\"borough\", \"STRING\"),\n", "    SchemaField(\"latitude\", \"NUMERIC\"),\n", "    SchemaField(\"longtitude\", \"NUMERIC\"),\n", "]\n", "\n", "BQ_TABLE = \"bus_stations\"\n", "\n", "dataset = bigquery.Dataset(f'{PROJECT_ID}.{STAGING_BQ_DATASET}')\n", "\n", "\n", "table_ref = dataset.table(BQ_TABLE)\n", "\n", "job_config = bigquery.LoadJobConfig(\n", "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n", "    source_format=bigquery.SourceFormat.CSV,\n", "    skip_leading_rows=1,\n", "    schema=bus_stations_schema,\n", ")\n", "with open(\"bus_stations.csv\", \"rb\") as fp:\n", "  load_job = bigquery_client.load_table_from_file(\n", "      fp, table_ref, job_config=job_config\n", "  )\n", "  load_job.result()\n", "\n", "print('created {}.{}'.format(STAGING_BQ_DATASET, BQ_TABLE))\n"], "metadata": {"id": "DKiM43tr95a8", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "81ceb5cb-ffbe-4a7a-80d0-65b6765b747d"}, "id": "DKiM43tr95a8", "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["# let's assume each bus starts a new route every"], "metadata": {"id": "Y0E8L8MmOjwO"}, "id": "Y0E8L8MmOjwO", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "id": "TqIu", "metadata": {"marimo": {"config": {"hide_code": true}}, "id": "TqIu"}, "source": ["## Extract data to GCS\n", "\n", "Now, that we have all data we need, the data needs to reside in GCS. We will extract the `ridership` BigQuery table\n", "to GCS as a CSV, we will store the local datasets (`bus_lines` & `fake_stations_lst`) we will save to local files\n", "(JSONL and CSV, respectively) and upload to GCS."], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "Vxnm", "metadata": {"id": "Vxnm", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "b77d6fa2-a923-400d-fcb1-a6a9be640394"}, "outputs": [], "source": ["target_glob = 'mta_staging_data/ridership/*.csv'\n", "destination_uri = 'gs://{}/{}'.format(BUCKET_NAME, target_glob)\n", "\n", "blob_list = bucket.list_blobs(match_glob=target_glob)\n", "blob_list = [blob for blob in blob_list]\n", "bucket.delete_blobs(blob_list)\n", "\n", "table_ref_1 = dataset.table('ridership')\n", "extract_job = bigquery_client.extract_table(table_ref_1, destination_uri, location=LOCATION)\n", "extract_job.result()"]}, {"cell_type": "code", "execution_count": null, "id": "ulZA", "metadata": {"id": "ulZA"}, "outputs": [], "source": ["bus_lines_blob = bucket.blob(\"mta_staging_data/bus_lines.json\")\n", "if bus_lines_blob.exists():\n", "  bus_lines_blob.delete()\n", "bus_lines_blob.upload_from_filename(\"bus_lines.json\")\n", "\n", "bus_stations_blob = bucket.blob(\"mta_staging_data/bus_stations.csv\")\n", "if bus_stations_blob.exists():\n", "  bus_stations_blob.delete()\n", "bus_stations_blob.upload_from_filename(\"bus_stations.csv\")"]}, {"cell_type": "code", "execution_count": null, "id": "ecfG", "metadata": {"id": "ecfG"}, "outputs": [], "source": ["# @title Drop tables and dataset, just to keep things clean\n", "# try:\n", "#   for table in bigquery_client.list_tables(dataset):\n", "#     bigquery_client.delete_table(table)\n", "#   bigquery_client.delete_dataset(dataset)\n", "# except exceptions.NotFound:\n", "#   print(\"Dataset looks already dropped\")"]}], "metadata": {"colab": {"provenance": [], "name": "lakehouse_part0_data_generation.ipynb"}, "language_info": {"name": "python"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 5}